{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dreamer V2\n",
    "\n",
    "\n",
    "In this workshop you'll get a short overview of model based reinforcement learning and Dreamer V2, a model based method that can achieve good results on both continuous and descrete tasks!\n",
    "\n",
    "\n",
    "This workshop is written using PyTorch and Pytorch Lightning\n",
    "\n",
    "\n",
    "Check out the paper, its original implementation, visualizations and other media at https://danijar.com/project/dreamerv2/\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explain the RSSM model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dreamer/lib/python3.9/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym.envs.atari'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, Dict, List, Tuple, TypeVar, Union, Optional\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdreamer_utils\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdreamer_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor\n",
      "File \u001B[0;32m~/AI/planet workshop/reinforcement-learning-workshop/Workshop/dreamer_utils/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01matari\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# so that you can do `from pydreamer import Dreamer`\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdmc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mminerl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/AI/planet workshop/reinforcement-learning-workshop/Workshop/dreamer_utils/atari.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mthreading\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01matari\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwrappers\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gym.envs.atari'"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from typing import Callable, Dict, List, Tuple, TypeVar, Union, Optional\n",
    "import numpy as np\n",
    "\n",
    "import dreamer_utils\n",
    "from dreamer_utils import *\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.distributions as D\n",
    "import torch.distributions as td\n",
    "import pytorch_lightning as pl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mRSSMCore\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, embed_dim, action_dim, deter_dim, stoch_dim, stoch_discrete, hidden_dim, gru_layers, gru_type, layer_norm):\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RSSMCore(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, action_dim, deter_dim, stoch_dim, stoch_discrete, hidden_dim, gru_layers, gru_type, layer_norm):\n",
    "        super().__init__()\n",
    "        self.cell = RSSMCell(embed_dim, action_dim, deter_dim, stoch_dim, stoch_discrete, hidden_dim, gru_layers, gru_type, layer_norm)\n",
    "\n",
    "    def forward(self,\n",
    "                embed: Tensor,       # tensor(T, B, E)\n",
    "                action: Tensor,      # tensor(T, B, A)\n",
    "                reset: Tensor,       # tensor(T, B)\n",
    "                in_state: Tuple[Tensor, Tensor],    # [(BI,D) (BI,S)]\n",
    "                iwae_samples: int = 1,\n",
    "                do_open_loop=False,\n",
    "                ):\n",
    "\n",
    "        T, B = embed.shape[:2]\n",
    "        I = iwae_samples\n",
    "\n",
    "        # Multiply batch dimension by I samples\n",
    "\n",
    "        def expand(x):\n",
    "            # (T,B,X) -> (T,BI,X)\n",
    "            return x.unsqueeze(2).expand(T, B, I, -1).reshape(T, B * I, -1)\n",
    "\n",
    "        embeds = expand(embed).unbind(0)     # (T,B,...) => List[(BI,...)]\n",
    "        actions = expand(action).unbind(0)\n",
    "        reset_masks = expand(~reset.unsqueeze(2)).unbind(0)\n",
    "\n",
    "        priors = []\n",
    "        posts = []\n",
    "        states_h = []\n",
    "        samples = []\n",
    "        (h, z) = in_state\n",
    "\n",
    "        for i in range(T):\n",
    "            if not do_open_loop:\n",
    "                post, (h, z) = self.cell.forward(embeds[i], actions[i], reset_masks[i], (h, z))\n",
    "            else:\n",
    "                post, (h, z) = self.cell.forward_prior(actions[i], reset_masks[i], (h, z))  # open loop: post=prior\n",
    "            posts.append(post)\n",
    "            states_h.append(h)\n",
    "            samples.append(z)\n",
    "\n",
    "        posts = torch.stack(posts)          # (T,BI,2S)\n",
    "        states_h = torch.stack(states_h)    # (T,BI,D)\n",
    "        samples = torch.stack(samples)      # (T,BI,S)\n",
    "        priors = self.cell.batch_prior(states_h)  # (T,BI,2S)\n",
    "        features = self.to_feature(states_h, samples)   # (T,BI,D+S)\n",
    "\n",
    "        posts = posts.reshape(T, B, I, -1)  # (T,BI,X) => (T,B,I,X)\n",
    "        states_h = states_h.reshape(T, B, I, -1)\n",
    "        samples = samples.reshape(T, B, I, -1)\n",
    "        priors = priors.reshape(T, B, I, -1)\n",
    "        states = (states_h, samples)\n",
    "        features = features.reshape(T, B, I, -1)\n",
    "\n",
    "        return (\n",
    "            priors,                      # tensor(T,B,I,2S)\n",
    "            posts,                       # tensor(T,B,I,2S)\n",
    "            samples,                     # tensor(T,B,I,S)\n",
    "            features,                    # tensor(T,B,I,D+S)\n",
    "            states,\n",
    "            (h.detach(), z.detach()),\n",
    "        )\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return self.cell.init_state(batch_size)\n",
    "\n",
    "    def to_feature(self, h: Tensor, z: Tensor) -> Tensor:\n",
    "        return torch.cat((h, z), -1)\n",
    "\n",
    "    def feature_replace_z(self, features: Tensor, z: Tensor):\n",
    "        h, _ = features.split([self.cell.deter_dim, z.shape[-1]], -1)\n",
    "        return self.to_feature(h, z)\n",
    "\n",
    "    def zdistr(self, pp: Tensor) -> D.Distribution:\n",
    "        return self.cell.zdistr(pp)\n",
    "\n",
    "\n",
    "class RSSMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, action_dim, deter_dim, stoch_dim, stoch_discrete, hidden_dim, gru_layers, gru_type, layer_norm):\n",
    "        super().__init__()\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.stoch_discrete = stoch_discrete\n",
    "        self.deter_dim = deter_dim\n",
    "        norm = nn.LayerNorm if layer_norm else NoNorm\n",
    "\n",
    "        self.z_mlp = nn.Linear(stoch_dim * (stoch_discrete or 1), hidden_dim)\n",
    "        self.a_mlp = nn.Linear(action_dim, hidden_dim, bias=False)  # No bias, because outputs are added\n",
    "        self.in_norm = norm(hidden_dim, eps=1e-3)\n",
    "\n",
    "        self.gru = dreamer_utils.GRUCellStack(hidden_dim, deter_dim, gru_layers, gru_type)\n",
    "\n",
    "        self.prior_mlp_h = nn.Linear(deter_dim, hidden_dim)\n",
    "        self.prior_norm = norm(hidden_dim, eps=1e-3)\n",
    "        self.prior_mlp = nn.Linear(hidden_dim, stoch_dim * (stoch_discrete or 2))\n",
    "\n",
    "        self.post_mlp_h = nn.Linear(deter_dim, hidden_dim)\n",
    "        self.post_mlp_e = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.post_norm = norm(hidden_dim, eps=1e-3)\n",
    "        self.post_mlp = nn.Linear(hidden_dim, stoch_dim * (stoch_discrete or 2))\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        device = next(self.gru.parameters()).device\n",
    "        return (\n",
    "            torch.zeros((batch_size, self.deter_dim), device=device),\n",
    "            torch.zeros((batch_size, self.stoch_dim * (self.stoch_discrete or 1)), device=device),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                embed: Tensor,                    # tensor(B,E)\n",
    "                action: Tensor,                   # tensor(B,A)\n",
    "                reset_mask: Tensor,               # tensor(B,1)\n",
    "                in_state: Tuple[Tensor, Tensor],\n",
    "                ) -> Tuple[Tensor,\n",
    "                           Tuple[Tensor, Tensor]]:\n",
    "\n",
    "        in_h, in_z = in_state\n",
    "        in_h = in_h * reset_mask\n",
    "        in_z = in_z * reset_mask\n",
    "        B = action.shape[0]\n",
    "\n",
    "        x = self.z_mlp(in_z) + self.a_mlp(action)  # (B,H)\n",
    "        x = self.in_norm(x)\n",
    "        za = F.elu(x)\n",
    "        h = self.gru(za, in_h)                                             # (B, D)\n",
    "\n",
    "        x = self.post_mlp_h(h) + self.post_mlp_e(embed)\n",
    "        x = self.post_norm(x)\n",
    "        post_in = F.elu(x)\n",
    "        post = self.post_mlp(post_in)                                    # (B, S*S)\n",
    "        post_distr = self.zdistr(post)\n",
    "        sample = post_distr.rsample().reshape(B, -1)\n",
    "\n",
    "        return (\n",
    "            post,                         # tensor(B, 2*S)\n",
    "            (h, sample),                  # tensor(B, D+S+G)\n",
    "        )\n",
    "\n",
    "    def forward_prior(self,\n",
    "                      action: Tensor,                   # tensor(B,A)\n",
    "                      reset_mask: Optional[Tensor],               # tensor(B,1)\n",
    "                      in_state: Tuple[Tensor, Tensor],  # tensor(B,D+S)\n",
    "                      ) -> Tuple[Tensor,\n",
    "                                 Tuple[Tensor, Tensor]]:\n",
    "\n",
    "        in_h, in_z = in_state\n",
    "        if reset_mask is not None:\n",
    "            in_h = in_h * reset_mask\n",
    "            in_z = in_z * reset_mask\n",
    "\n",
    "        B = action.shape[0]\n",
    "\n",
    "        x = self.z_mlp(in_z) + self.a_mlp(action)  # (B,H)\n",
    "        x = self.in_norm(x)\n",
    "        za = F.elu(x)\n",
    "        h = self.gru(za, in_h)                  # (B, D)\n",
    "\n",
    "        x = self.prior_mlp_h(h)\n",
    "        x = self.prior_norm(x)\n",
    "        x = F.elu(x)\n",
    "        prior = self.prior_mlp(x)          # (B,2S)\n",
    "        prior_distr = self.zdistr(prior)\n",
    "        sample = prior_distr.rsample().reshape(B, -1)\n",
    "\n",
    "        return (\n",
    "            prior,                        # (B,2S)\n",
    "            (h, sample),                  # (B,D+S)\n",
    "        )\n",
    "\n",
    "    def batch_prior(self,\n",
    "                    h: Tensor,     # tensor(T, B, D)\n",
    "                    ) -> Tensor:\n",
    "        x = self.prior_mlp_h(h)\n",
    "        x = self.prior_norm(x)\n",
    "        x = F.elu(x)\n",
    "        prior = self.prior_mlp(x)  # tensor(B,2S)\n",
    "        return prior\n",
    "\n",
    "    def zdistr(self, pp: Tensor) -> D.Distribution:\n",
    "        # pp = post or prior\n",
    "        if self.stoch_discrete:\n",
    "            logits = pp.reshape(pp.shape[:-1] + (self.stoch_dim, self.stoch_discrete))\n",
    "            distr = D.OneHotCategoricalStraightThrough(logits=logits.float())  # NOTE: .float() needed to force float32 on AMP\n",
    "            distr = D.independent.Independent(distr, 1)  # This makes d.entropy() and d.kl() sum over stoch_dim\n",
    "            return distr\n",
    "        else:\n",
    "            return dreamer_utils.diag_normal(pp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explain the vision models"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMultiEncoder\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, conf):\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "        self.reward_input = conf.reward_input\n",
    "        if conf.reward_input:\n",
    "            encoder_channels = conf.image_channels + 2  # + reward, terminal\n",
    "        else:\n",
    "            encoder_channels = conf.image_channels\n",
    "\n",
    "        if conf.image_encoder == 'cnn':\n",
    "            self.encoder_image = ConvEncoder(in_channels=encoder_channels,\n",
    "                                             cnn_depth=conf.cnn_depth)\n",
    "        elif conf.image_encoder == 'dense':\n",
    "            self.encoder_image = DenseEncoder(in_dim=conf.image_size * conf.image_size * encoder_channels,\n",
    "                                              out_dim=256,\n",
    "                                              hidden_layers=conf.image_encoder_layers,\n",
    "                                              layer_norm=conf.layer_norm)\n",
    "        elif not conf.image_encoder:\n",
    "            self.encoder_image = None\n",
    "        else:\n",
    "            assert False, conf.image_encoder\n",
    "\n",
    "        if conf.vecobs_size:\n",
    "            self.encoder_vecobs = MLP(conf.vecobs_size, 256, hidden_dim=400, hidden_layers=2, layer_norm=conf.layer_norm)\n",
    "        else:\n",
    "            self.encoder_vecobs = None\n",
    "\n",
    "        assert self.encoder_image or self.encoder_vecobs, \"Either image_encoder or vecobs_size should be set\"\n",
    "        self.out_dim = ((self.encoder_image.out_dim if self.encoder_image else 0) +\n",
    "                        (self.encoder_vecobs.out_dim if self.encoder_vecobs else 0))\n",
    "\n",
    "    def forward(self, obs: Dict[str, Tensor]) -> dreamer_utils.TensorTBE:\n",
    "        # TODO:\n",
    "        #  1) Make this more generic, e.g. working without image input or without vecobs\n",
    "        #  2) Treat all inputs equally, adding everything via linear layer to embed_dim\n",
    "\n",
    "        embeds = []\n",
    "\n",
    "        if self.encoder_image:\n",
    "            image = obs['image']\n",
    "            T, B, C, H, W = image.shape\n",
    "            if self.reward_input:\n",
    "                reward = obs['reward']\n",
    "                terminal = obs['terminal']\n",
    "                reward_plane = reward.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand((T, B, 1, H, W))\n",
    "                terminal_plane = terminal.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand((T, B, 1, H, W))\n",
    "                image = torch.cat([image,  # (T,B,C+2,H,W)\n",
    "                                reward_plane.to(image.dtype),\n",
    "                                terminal_plane.to(image.dtype)], dim=-3)\n",
    "\n",
    "            embed_image = self.encoder_image.forward(image)  # (T,B,E)\n",
    "            embeds.append(embed_image)\n",
    "\n",
    "        if self.encoder_vecobs:\n",
    "            embed_vecobs = self.encoder_vecobs(obs['vecobs'])\n",
    "            embeds.append(embed_vecobs)\n",
    "\n",
    "        embed = torch.cat(embeds, dim=-1)  # (T,B,E+256)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, cnn_depth=32, activation=nn.ELU):\n",
    "        super().__init__()\n",
    "        self.out_dim = cnn_depth * 32\n",
    "        kernels = (4, 4, 4, 4)\n",
    "        stride = 2\n",
    "        d = cnn_depth\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, d, kernels[0], stride),\n",
    "            activation(),\n",
    "            nn.Conv2d(d, d * 2, kernels[1], stride),\n",
    "            activation(),\n",
    "            nn.Conv2d(d * 2, d * 4, kernels[2], stride),\n",
    "            activation(),\n",
    "            nn.Conv2d(d * 4, d * 8, kernels[3], stride),\n",
    "            activation(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, bd = dreamer_utils.flatten_batch(x, 3)\n",
    "        y = self.model(x)\n",
    "        y = dreamer_utils.unflatten_batch(y, bd)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DenseEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=256, activation=nn.ELU, hidden_dim=400, hidden_layers=2, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        norm = nn.LayerNorm if layer_norm else dreamer_utils.NoNorm\n",
    "        layers = [nn.Flatten()]\n",
    "        layers += [\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            norm(hidden_dim, eps=1e-3),\n",
    "            activation()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers += [\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                norm(hidden_dim, eps=1e-3),\n",
    "                activation()]\n",
    "        layers += [\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            activation()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, bd = dreamer_utils.flatten_batch(x, 3)\n",
    "        y = self.model(x)\n",
    "        y = dreamer_utils.unflatten_batch(y, bd)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class MultiDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features_dim, conf):\n",
    "        super().__init__()\n",
    "        self.image_weight = conf.image_weight\n",
    "        self.vecobs_weight = conf.vecobs_weight\n",
    "        self.reward_weight = conf.reward_weight\n",
    "        self.terminal_weight = conf.terminal_weight\n",
    "\n",
    "        if conf.image_decoder == 'cnn':\n",
    "            self.image = ConvDecoder(in_dim=features_dim,\n",
    "                                     out_channels=conf.image_channels,\n",
    "                                     cnn_depth=conf.cnn_depth)\n",
    "        elif conf.image_decoder == 'dense':\n",
    "            self.image = CatImageDecoder(in_dim=features_dim,\n",
    "                                         out_shape=(conf.image_channels, conf.image_size, conf.image_size),\n",
    "                                         hidden_layers=conf.image_decoder_layers,\n",
    "                                         layer_norm=conf.layer_norm,\n",
    "                                         min_prob=conf.image_decoder_min_prob)\n",
    "        elif not conf.image_decoder:\n",
    "            self.image = None\n",
    "        else:\n",
    "            assert False, conf.image_decoder\n",
    "\n",
    "        if conf.reward_decoder_categorical:\n",
    "            self.reward = DenseCategoricalSupportDecoder(in_dim=features_dim,\n",
    "                                                         support=conf.reward_decoder_categorical,\n",
    "                                                         hidden_layers=conf.reward_decoder_layers,\n",
    "                                                         layer_norm=conf.layer_norm)\n",
    "        else:\n",
    "            self.reward = DenseNormalDecoder(in_dim=features_dim, hidden_layers=conf.reward_decoder_layers, layer_norm=conf.layer_norm)\n",
    "\n",
    "        self.terminal = DenseBernoulliDecoder(in_dim=features_dim, hidden_layers=conf.terminal_decoder_layers, layer_norm=conf.layer_norm)\n",
    "\n",
    "        if conf.vecobs_size:\n",
    "            self.vecobs = DenseNormalDecoder(in_dim=features_dim, out_dim=conf.vecobs_size, hidden_layers=4, layer_norm=conf.layer_norm)\n",
    "        else:\n",
    "            self.vecobs = None\n",
    "\n",
    "    def training_step(self,\n",
    "                      features: dreamer_utils.TensorTBIF,\n",
    "                      obs: Dict[str, Tensor],\n",
    "                      extra_metrics: bool = False\n",
    "                      ) -> Tuple[ dreamer_utils.TensorTBI, Dict[str, Tensor], Dict[str, Tensor]]:\n",
    "        tensors = {}\n",
    "        metrics = {}\n",
    "        loss_reconstr = 0\n",
    "\n",
    "        if self.image:\n",
    "            loss_image_tbi, loss_image, image_rec = self.image.training_step(features, obs['image'])\n",
    "            loss_reconstr += self.image_weight * loss_image_tbi\n",
    "            metrics.update(loss_image=loss_image.detach().mean())\n",
    "            tensors.update(loss_image=loss_image.detach(),\n",
    "                        image_rec=image_rec.detach())\n",
    "\n",
    "        if self.vecobs:\n",
    "            loss_vecobs_tbi, loss_vecobs, vecobs_rec = self.vecobs.training_step(features, obs['vecobs'])\n",
    "            loss_reconstr += self.vecobs_weight * loss_vecobs_tbi\n",
    "            metrics.update(loss_vecobs=loss_vecobs.detach().mean())\n",
    "            tensors.update(loss_vecobs=loss_vecobs.detach(),\n",
    "                        vecobs_rec=vecobs_rec.detach())\n",
    "\n",
    "        loss_reward_tbi, loss_reward, reward_rec = self.reward.training_step(features, obs['reward'])\n",
    "        loss_reconstr += self.reward_weight * loss_reward_tbi\n",
    "        metrics.update(loss_reward=loss_reward.detach().mean())\n",
    "        tensors.update(loss_reward=loss_reward.detach(),\n",
    "                       reward_rec=reward_rec.detach())\n",
    "\n",
    "        loss_terminal_tbi, loss_terminal, terminal_rec = self.terminal.training_step(features, obs['terminal'])\n",
    "        loss_reconstr += self.terminal_weight * loss_terminal_tbi\n",
    "        metrics.update(loss_terminal=loss_terminal.detach().mean())\n",
    "        tensors.update(loss_terminal=loss_terminal.detach(),\n",
    "                       terminal_rec=terminal_rec.detach())\n",
    "\n",
    "        if extra_metrics:\n",
    "            mask_rewardp = obs['reward'] > 0  # mask where reward is positive\n",
    "            loss_rewardp = loss_reward * mask_rewardp / mask_rewardp  # set to nan where ~mask\n",
    "            metrics.update(loss_rewardp= dreamer_utils.nanmean(loss_rewardp))\n",
    "            tensors.update(loss_rewardp=loss_rewardp)\n",
    "\n",
    "            mask_rewardn = obs['reward'] < 0  # mask where reward is negative\n",
    "            loss_rewardn = loss_reward * mask_rewardn / mask_rewardn  # set to nan where ~mask\n",
    "            metrics.update(loss_rewardn= dreamer_utils.nanmean(loss_rewardn))\n",
    "            tensors.update(loss_rewardn=loss_rewardn)\n",
    "\n",
    "            mask_terminal1 = obs['terminal'] > 0  # mask where terminal is 1\n",
    "            loss_terminal1 = loss_terminal * mask_terminal1 / mask_terminal1  # set to nan where ~mask\n",
    "            metrics.update(loss_terminal1= dreamer_utils.nanmean(loss_terminal1))\n",
    "            tensors.update(loss_terminal1=loss_terminal1)\n",
    "\n",
    "        return loss_reconstr, metrics, tensors\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_channels=3,\n",
    "                 cnn_depth=32,\n",
    "                 mlp_layers=0,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.ELU\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        kernels = (5, 5, 6, 6)\n",
    "        stride = 2\n",
    "        d = cnn_depth\n",
    "        if mlp_layers == 0:\n",
    "            layers = [\n",
    "                nn.Linear(in_dim, d * 32),  # No activation here in DreamerV2\n",
    "            ]\n",
    "        else:\n",
    "            hidden_dim = d * 32\n",
    "            norm = nn.LayerNorm if layer_norm else  dreamer_utils.NoNorm\n",
    "            layers = [\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                norm(hidden_dim, eps=1e-3),\n",
    "                activation()\n",
    "            ]\n",
    "            for _ in range(mlp_layers - 1):\n",
    "                layers += [\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    norm(hidden_dim, eps=1e-3),\n",
    "                    activation()]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # FC\n",
    "            *layers,\n",
    "            nn.Unflatten(-1, (d * 32, 1, 1)),  # type: ignore\n",
    "            # Deconv\n",
    "            nn.ConvTranspose2d(d * 32, d * 4, kernels[0], stride),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(d * 4, d * 2, kernels[1], stride),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(d * 2, d, kernels[2], stride),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(d, out_channels, kernels[3], stride))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, bd =  dreamer_utils.flatten_batch(x)\n",
    "        y = self.model(x)\n",
    "        y =  dreamer_utils.unflatten_batch(y, bd)\n",
    "        return y\n",
    "\n",
    "    def loss(self, output: Tensor, target: Tensor) -> Tensor:\n",
    "        output, bd =  dreamer_utils.flatten_batch(output, 3)\n",
    "        target, _ =  dreamer_utils.flatten_batch(target, 3)\n",
    "        loss = 0.5 * torch.square(output - target).sum(dim=[-1, -2, -3])  # MSE\n",
    "        return  dreamer_utils.unflatten_batch(loss, bd)\n",
    "\n",
    "    def training_step(self, features:  dreamer_utils.TensorTBIF, target:  dreamer_utils.TensorTBCHW) -> Tuple[ dreamer_utils.TensorTBI,  dreamer_utils.TensorTB,  dreamer_utils.TensorTBCHW]:\n",
    "        assert len(features.shape) == 4 and len(target.shape) == 5\n",
    "        I = features.shape[2]\n",
    "        target =  dreamer_utils.insert_dim(target, 2, I)  # Expand target with iwae_samples dim, because features have it\n",
    "\n",
    "        decoded = self.forward(features)\n",
    "        loss_tbi = self.loss(decoded, target)\n",
    "        loss_tb = - dreamer_utils.logavgexp(-loss_tbi, dim=2)  # TBI => TB\n",
    "        decoded = decoded.mean(dim=2)  # TBICHW => TBCHW\n",
    "\n",
    "        assert len(loss_tbi.shape) == 3 and len(decoded.shape) == 5\n",
    "        return loss_tbi, loss_tb, decoded\n",
    "\n",
    "\n",
    "class CatImageDecoder(nn.Module):\n",
    "    \"\"\"Dense decoder for categorical image, e.g. map\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_shape=(33, 7, 7), activation=nn.ELU, hidden_dim=400, hidden_layers=2, layer_norm=True, min_prob=0):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_shape = out_shape\n",
    "        norm = nn.LayerNorm if layer_norm else  dreamer_utils.NoNorm\n",
    "        layers = []\n",
    "        layers += [\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            norm(hidden_dim, eps=1e-3),\n",
    "            activation()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers += [\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                norm(hidden_dim, eps=1e-3),\n",
    "                activation()]\n",
    "        layers += [\n",
    "            nn.Linear(hidden_dim, np.prod(out_shape)),\n",
    "            nn.Unflatten(-1, out_shape)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.min_prob = min_prob\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, bd = flatten_batch(x)\n",
    "        y = self.model(x)\n",
    "        y = unflatten_batch(y, bd)\n",
    "        return y\n",
    "\n",
    "    def loss(self, output: Tensor, target: Tensor) -> Tensor:\n",
    "        if len(output.shape) == len(target.shape):\n",
    "            target = target.argmax(dim=-3)  # float(*,C,H,W) => int(*,H,W)\n",
    "        assert target.dtype == torch.int64, 'Target should be categorical'\n",
    "        output, bd = flatten_batch(output, len(self.out_shape))     # (*,C,H,W) => (B,C,H,W)\n",
    "        target, _ = flatten_batch(target, len(self.out_shape) - 1)  # (*,H,W) => (B,H,W)\n",
    "\n",
    "        if self.min_prob == 0:\n",
    "            loss = F.nll_loss(F.log_softmax(output, 1), target, reduction='none')  # = F.cross_entropy()\n",
    "        else:\n",
    "            prob = F.softmax(output, 1)\n",
    "            prob = (1.0 - self.min_prob) * prob + self.min_prob * (1.0 / prob.size(1))  # mix with uniform prob\n",
    "            loss = F.nll_loss(prob.log(), target, reduction='none')\n",
    "\n",
    "        if len(self.out_shape) == 3:\n",
    "            loss = loss.sum(dim=[-1, -2])  # (*,H,W) => (*)\n",
    "        assert len(loss.shape) == 1\n",
    "        return unflatten_batch(loss, bd)\n",
    "\n",
    "    def training_step(self, features: TensorTBIF, target: TensorTBCHW) -> Tuple[TensorTBI, TensorTB, TensorTBCHW]:\n",
    "        assert len(features.shape) == 4 and len(target.shape) == 5\n",
    "        I = features.shape[2]\n",
    "        target = insert_dim(target, 2, I)  # Expand target with iwae_samples dim, because features have it\n",
    "\n",
    "        logits = self.forward(features)\n",
    "        loss_tbi = self.loss(logits, target)\n",
    "        loss_tb = -logavgexp(-loss_tbi, dim=2)  # TBI => TB\n",
    "\n",
    "        assert len(logits.shape) == 6   # TBICHW\n",
    "        logits = logits - logits.logsumexp(dim=-3, keepdim=True)  # normalize C\n",
    "        logits = torch.logsumexp(logits, dim=2)  # aggregate I => TBCHW\n",
    "        logits = logits - logits.logsumexp(dim=-3, keepdim=True)  # normalize C again\n",
    "        decoded = logits\n",
    "\n",
    "        assert len(loss_tbi.shape) == 3 and len(decoded.shape) == 5\n",
    "        return loss_tbi, loss_tb, decoded\n",
    "\n",
    "\n",
    "class DenseBernoulliDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim=400, hidden_layers=2, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.model = MLP(in_dim, 1, hidden_dim, hidden_layers, layer_norm)\n",
    "\n",
    "    def forward(self, features: Tensor) -> D.Distribution:\n",
    "        y = self.model.forward(features)\n",
    "        p = D.Bernoulli(logits=y.float())\n",
    "        return p\n",
    "\n",
    "    def loss(self, output: D.Distribution, target: Tensor) -> Tensor:\n",
    "        return -output.log_prob(target)\n",
    "\n",
    "    def training_step(self, features: TensorTBIF, target: Tensor) -> Tuple[TensorTBI, TensorTB, TensorTB]:\n",
    "        assert len(features.shape) == 4\n",
    "        I = features.shape[2]\n",
    "        target = insert_dim(target, 2, I)  # Expand target with iwae_samples dim, because features have it\n",
    "\n",
    "        decoded = self.forward(features)\n",
    "        loss_tbi = self.loss(decoded, target)\n",
    "        loss_tb = -logavgexp(-loss_tbi, dim=2)  # TBI => TB\n",
    "        decoded = decoded.mean.mean(dim=2)\n",
    "\n",
    "        assert len(loss_tbi.shape) == 3\n",
    "        assert len(loss_tb.shape) == 2\n",
    "        assert len(decoded.shape) == 2\n",
    "        return loss_tbi, loss_tb, decoded\n",
    "\n",
    "\n",
    "class DenseNormalDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=1, hidden_dim=400, hidden_layers=2, layer_norm=True, std=0.3989422804):\n",
    "        super().__init__()\n",
    "        self.model = MLP(in_dim, out_dim, hidden_dim, hidden_layers, layer_norm)\n",
    "        self.std = std\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, features: Tensor) -> D.Distribution:\n",
    "        y = self.model.forward(features)\n",
    "        p = D.Normal(loc=y, scale=torch.ones_like(y) * self.std)\n",
    "        if self.out_dim > 1:\n",
    "            p = D.independent.Independent(p, 1)  # Makes p.logprob() sum over last dim\n",
    "        return p\n",
    "\n",
    "    def loss(self, output: D.Distribution, target: Tensor) -> Tensor:\n",
    "        var = self.std ** 2  # var cancels denominator, which makes loss = 0.5 (target-output)^2\n",
    "        return -output.log_prob(target) * var\n",
    "\n",
    "    def training_step(self, features: TensorTBIF, target: Tensor) -> Tuple[TensorTBI, TensorTB, Tensor]:\n",
    "        assert len(features.shape) == 4\n",
    "        I = features.shape[2]\n",
    "        target = insert_dim(target, 2, I)  # Expand target with iwae_samples dim, because features have it\n",
    "\n",
    "        decoded = self.forward(features)\n",
    "        loss_tbi = self.loss(decoded, target)\n",
    "        loss_tb = -logavgexp(-loss_tbi, dim=2)  # TBI => TB\n",
    "        decoded = decoded.mean.mean(dim=2)\n",
    "\n",
    "        assert len(loss_tbi.shape) == 3\n",
    "        assert len(loss_tb.shape) == 2\n",
    "        assert len(decoded.shape) == (2 if self.out_dim == 1 else 3)\n",
    "        return loss_tbi, loss_tb, decoded\n",
    "\n",
    "\n",
    "class DenseCategoricalSupportDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Represent continuous variable distribution by discrete set of support values.\n",
    "    Useful for reward head, which can be e.g. [-10, 0, 1, 10]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, support=[0.0, 1.0], hidden_dim=400, hidden_layers=2, layer_norm=True):\n",
    "        assert isinstance(support, list)\n",
    "        super().__init__()\n",
    "        self.model = MLP(in_dim, len(support), hidden_dim, hidden_layers, layer_norm)\n",
    "        self.support = nn.Parameter(torch.tensor(support), requires_grad=False)\n",
    "\n",
    "    def forward(self, features: Tensor) -> D.Distribution:\n",
    "        y = self.model.forward(features)\n",
    "        p = CategoricalSupport(logits=y.float(), support=self.support.data)\n",
    "        return p\n",
    "\n",
    "    def loss(self, output: D.Distribution, target: Tensor) -> Tensor:\n",
    "        target = self.to_categorical(target)\n",
    "        return -output.log_prob(target)\n",
    "\n",
    "    def to_categorical(self, target: Tensor) -> Tensor:\n",
    "        # TODO: should interpolate between adjacent values, like in MuZero\n",
    "        distances = torch.square(target.unsqueeze(-1) - self.support)\n",
    "        return distances.argmin(-1)\n",
    "\n",
    "    def training_step(self, features: TensorTBIF, target: Tensor) -> Tuple[TensorTBI, TensorTB, TensorTB]:\n",
    "        assert len(features.shape) == 4\n",
    "        I = features.shape[2]\n",
    "        target = insert_dim(target, 2, I)  # Expand target with iwae_samples dim, because features have it\n",
    "\n",
    "        decoded = self.forward(features)\n",
    "        loss_tbi = self.loss(decoded, target)\n",
    "        loss_tb = -logavgexp(-loss_tbi, dim=2)  # TBI => TB\n",
    "        decoded = decoded.mean.mean(dim=2)\n",
    "\n",
    "        assert len(loss_tbi.shape) == 3\n",
    "        assert len(loss_tb.shape) == 2\n",
    "        assert len(decoded.shape) == 2\n",
    "        return loss_tbi, loss_tb, decoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explain the dense models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explain the actor models\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
