{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxFM9e_pzcOz"
      },
      "source": [
        "# Deep Q-learning\n",
        "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4la1EczcO0"
      },
      "source": [
        "Deep Reinforcement learning has resulted in superman performance in a range of scenarios, such as Chess, Atari 2600, and Starcraft, but also in robotics. The main reason for this success compared to traditional Reinforcement Learning is the introduction of deep neural networks. The first successful implementation of deep neural networks in combination with a Reinforcement learning algorithm was Mnih et. al. which was able to reach superhuman performance in the classic Atari games. Their approach was coined Deep Q-Networks(DQN), and uses the Q-learning algorithm we used in the previous notebook, in combination with deep neural networks.\n",
        "\n",
        "In the DQN approach, a neural network replaces the Q-table which we used in the previous notebook. The main reason is the neural networks ability to generalize around similar states. In a continuous environment, a Q-table would quickly grow to tens of millions of unique states, although many of these are so similar that the same action should be applied. This is both an issue due to the huge table we would need, but also because we would need a tremendous amount of training data to update all the states. \n",
        "\n",
        "Neural networks address this issue, but come with their own challenges. Neural network struggles with instabilities during training, in addition to critical forgetting and divergence. Mnih's success was due to several ingenious tricks which stabilized the network during training, and we will go through these during the workshop.\n",
        "\n",
        "In the previous notebook, we created a lot of methods, which resulted in many parameters being passed from method to method. This can quickly become convoluted, and in this notebook we instead use classes. An example can be seen below. When the class is created the init method is run, and the different parameters are set and stored in this instance of the class. The parameters can be called within the class using \"self.<name_of_parameter>\" syntax, e.g. self.learning_rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTRRil4RzcO1"
      },
      "source": [
        "## Import packages and create support methods for workshop\n",
        "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the notebook. \n",
        "\n",
        "### Task 1\n",
        "Import the packages needed for this workshop, simply mark the cell below and press CTRL + Enter\n",
        "\n",
        "### Task 2\n",
        "Create the necessary support methods by running the second cell below. Mark it and press CTRL + Enter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "KFRH2s0mz8vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cb8Tww7zcO1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import logging\n",
        "import os\n",
        "import datetime\n",
        "import gym\n",
        "import io\n",
        "import base64\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from statistics import mean\n",
        "from IPython.display import HTML\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.metrics import MSE\n",
        "from tensorflow.python.keras.optimizers import adam_v2\n",
        "from tensorflow.python.keras.initializers import glorot_uniform\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "if not os.path.exists(\"videos\"):\n",
        "    os.makedirs(\"videos\")\n",
        "    \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "%precision 3\n",
        "should_assert = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcuWU8XWzcO3"
      },
      "outputs": [],
      "source": [
        "def generate_blank_weights(layer_dims):\n",
        "    \n",
        "    # Xavier/Glorot Initialization\n",
        "    new_weights = np.random.randn(layer_dims[0], layer_dims[1])*np.sqrt(1/layer_dims[0])\n",
        "    new_bias = np.zeros((layer_dims[1]))\n",
        "    \n",
        "    return [new_weights, new_bias]\n",
        "\n",
        "def create_dummybuffer():\n",
        "    mock_env = gym.make(\"CartPole-v1\")\n",
        "    parameters = {\"buffer_size\": 6,\n",
        "                  \"batch_size\": 3}\n",
        "    DummyBuffer = ExperienceReplay(mock_env, parameters)\n",
        "    \n",
        "    return DummyBuffer\n",
        "\n",
        "def get_dummy_parameters_and_env():\n",
        "        \n",
        "        parameters = {\n",
        "            \"tau\" : 0.4,\n",
        "            \"gamma\" : 1,\n",
        "            \"epsilon_init\" : 1,\n",
        "            \"epsilon_decay\" : 1,\n",
        "            \"epsilon_minimum\": 0.01,\n",
        "            \"buffer_size\" : 2000,\n",
        "            \"batch_size\" : 64,\n",
        "            \"epochs\": 1,\n",
        "            \"loss_metric\" : \"mse\",\n",
        "            \"learning_rate\" : 0.01,\n",
        "            \"learning_rate_decay\": 0.01,\n",
        "            \"hidden_layer_1\": 5,\n",
        "            \"hidden_layer_2\": 5}\n",
        "        \n",
        "        dummy_env = gym.make(\"CartPole-v0\")\n",
        "        return parameters, dummy_env \n",
        "    \n",
        "def clear_video_folder():\n",
        "    video_path = \"videos\"\n",
        "    for item in os.listdir(video_path):\n",
        "        os.remove(os.path.join(video_path, item))\n",
        "\n",
        "def play(agent):\n",
        "    \n",
        "    old_epsilon = agent.epsilon\n",
        "    \n",
        "    done = False\n",
        "    agent.epsilon = 0\n",
        "    total_reward = 0\n",
        "    state = agent.env.reset()\n",
        "    \n",
        "    while not done:\n",
        "        action, reward, done, new_state = agent.step(state)\n",
        "        state = new_state\n",
        "        \n",
        "        total_reward += reward\n",
        "    print(\"Total Reward: {}\".format(total_reward))    \n",
        "    \n",
        "    agent.epsilon = old_epsilon\n",
        "    \n",
        "def generate_video(agent):\n",
        "    \n",
        "    if os.listdir(\"./videos/\"):\n",
        "        clear_video_folder()\n",
        "\n",
        "    monitor = gym.wrappers.Monitor(agent.env, directory=\"videos\", force=True)\n",
        "    original_env = agent.env\n",
        "    agent.env = monitor\n",
        "    \n",
        "    old_epsilon = agent.epsilon\n",
        "    \n",
        "    for _ in range(15):\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        agent.epsilon = 0\n",
        "        total_reward = 0\n",
        "        state = agent.env.reset()\n",
        "        while not done:\n",
        "            action, reward, done, new_state = agent.step(state)\n",
        "            state = new_state\n",
        "\n",
        "            total_reward += reward\n",
        "    \n",
        "    print(\"Total Reward: {}\".format(total_reward))    \n",
        "    agent.epsilon = old_epsilon\n",
        "    agent.env = original_env\n",
        "    monitor.close()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('videos/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0zcrsHCzcO4"
      },
      "source": [
        "## Create Q-Networks\n",
        "There are several frameworks which can be used to create neural networks such as Tensorflow from Google, PyTorch from Facebook or MXNet from Apache. In this notebook, we will be using Tensorflow, with a high-level API called Keras on top. Keras greatly simplifies the effort needed to create and train a neural network, and it is often referred to as the best Deep Learning framework for those who are just starting with neural networks.\n",
        "\n",
        "Neural networks are partially inspired by the structure of the human brain and consist of a network of interconnected neurons. The neural network consists of an input layer, a set of hidden layers, and an output layer, as shown in the figure below.\n",
        "\n",
        "<img src=\"https://github.com/acntech/reinforcement-learning-workshop/blob/master/Workshop/Images/Neural_network__achitecture.svg?raw=1\" alt=\"drawing\" width=\"400\" height=\"200\"/>\n",
        "\n",
        "\n",
        "\n",
        "### Task\n",
        "Now we will experience how easy it is to create a neural network using Keras. We are going to create a neural network with an input layer, two hidden layers, and one output layer. We have completed steps 1,2, 5, and your task is to complete task 3 and 4 according to the design criterions below: <br>\n",
        "\n",
        "1. The model should be fully connected \n",
        "<br>\n",
        "<br>\n",
        "2. The input layer and first hidden layer is created together, and should:\n",
        "    - Be of type Dense\n",
        "    - Use the ReLU activation function\n",
        "    - Have input size equal to the observations size of the environment\n",
        "    - Have hidden layers size equal to parameter hidden_layer_1 \n",
        "<br>\n",
        "<br>\n",
        "3. The second hidden layer should:\n",
        "    - Be of type Dense\n",
        "    - Use the ReLU activation function \n",
        "    - The hidden layer should have size equal to parameter hidden_layer_2\n",
        "    - Tip1: This layer is similar to previous, but without the input_dim parameter\n",
        "    - Tip2: Remember to add the dense layer using model.add()\n",
        "<br>\n",
        "<br>\n",
        "4. The output layer should:\n",
        "    - Be of type Dense\n",
        "    - The activation function should be linear\n",
        "    - Should have size equal to action vector, i.e. action_size\n",
        "<br>\n",
        "<br>\n",
        "5. The final part is to compile the network. Before we do this we must define som parameters\n",
        "    - Loss metric should be Mean-squared-error (MSE)\n",
        "    - The optimizer should be Adaptive Moment Estimation (Adam)\n",
        "    - Learning rate should equal to learning_rate\n",
        "    - Learning rate decay should be equal to learning_rate_decay\n",
        "<br>\n",
        "\n",
        "**Ps!** <br>\n",
        "At the end of the workshop, you can experiment with different structures and parameters, however, in the first walkthrough, you will follow the defined steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMYiHh41zcO4"
      },
      "outputs": [],
      "source": [
        "class QNetwork:\n",
        "\n",
        "    def __init__(self, env, parameters):\n",
        "        self.observations_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.learning_rate = parameters[\"learning_rate\"]\n",
        "        self.learning_rate_decay = parameters[\"learning_rate_decay\"]\n",
        "        self.loss_metric = parameters[\"loss_metric\"]\n",
        "        self.hidden_layer_1 = parameters[\"hidden_layer_1\"]\n",
        "        self.hidden_layer_2 = parameters[\"hidden_layer_2\"]    \n",
        "        \n",
        "    def build_q_network(self):\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.hidden_layer_1, input_dim=self.observations_size, activation='relu'))\n",
        "        \n",
        "        \"Input code below\"\n",
        "        \n",
        "        \"Input code above\"\n",
        "        \n",
        "        model.compile(loss=self.loss_metric, optimizer=adam_v2.Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
        "    \n",
        "        return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTxKCjlGzcO4",
        "outputId": "db4640c9-dedb-4686-f11f-69de738f4f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'sequential', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 4), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_input'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 4), 'dtype': 'float32', 'units': 24, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 24, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "{'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 4), 'dtype': 'float32', 'units': 24, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}\n",
            "{'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 24, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}\n",
            "{'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}\n",
            "{'name': 'Adam', 'learning_rate': 0.01, 'decay': 0.01, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "Superb, you implemented the layers correct! Information about your model is shown below.\n",
            "\n",
            "PS. The input layer is not shown, altough you should be able to calculate it by looking at <Param #>\n",
            " of the first hidden layer.\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 24)                120       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Do not edit - Assertion cell #\n",
        "if(should_assert):\n",
        "    mock_parameters = {\"loss_metric\" : \"mse\",\n",
        "                \"learning_rate\" : 0.01,\n",
        "                \"learning_rate_decay\": 0.01,\n",
        "                \"hidden_layer_1\": 24,\n",
        "                \"hidden_layer_2\": 24}\n",
        "\n",
        "    mock_env = gym.make(\"CartPole-v0\")\n",
        "    mock_q_network = QNetwork(mock_env, mock_parameters)\n",
        "    mock_model = mock_q_network.build_q_network()\n",
        "    config_network = mock_model.get_config()\n",
        "    assert(config_network.get(\"name\")[0:10] == \"sequential\")\n",
        "\n",
        "    # Layers\n",
        "    config_layers = config_network.get(\"layers\")\n",
        "    assert(len(config_layers) == 4), f\"You should only have 2 dense layers, you've got {len(config_layers)}\"\n",
        "\n",
        "    # First layer\n",
        "    assert(config_layers[0].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
        "    assert(config_layers[0].get(\"config\").get(\"batch_input_shape\") == (None, mock_q_network.observations_size))\n",
        "    assert(config_layers[0].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
        "    assert(config_layers[0].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
        "    \"Activation function for first layer should be relu\"\n",
        "\n",
        "\n",
        "    # Second layer\n",
        "    assert(config_layers[1].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
        "    assert(config_layers[1].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
        "    assert(config_layers[1].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
        "    \"Activation function for second layer should be relu\"\n",
        "\n",
        "    # Thrid layer\n",
        "    assert(config_layers[2].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
        "    assert(config_layers[2].get(\"config\").get(\"units\") == mock_q_network.action_size), \\\n",
        "    \"Incorrect number of neurons in first layer\"\n",
        "    assert(config_layers[2].get(\"config\").get(\"activation\") == \"linear\"),\\\n",
        "    \"Activation function for third layer should be linear\"\n",
        "\n",
        "    config_optimizer = mock_model.optimizer.get_config()\n",
        "    print(config_optimizer)\n",
        "    assert(0.0099 < config_optimizer.get(\"learning_rate\") <= 0.01),\"Learning rate should be 0.01\"\n",
        "    assert(0.0099 < config_optimizer.get(\"decay\") <= 0.01),\"Learning rate decay should be 0.01\"\n",
        "    assert(mock_model.loss == \"mse\"), \"Loss metric should me mse\"\n",
        "    \n",
        "    print(\"Superb, you implemented the layers correct! Information about your model is shown below.\\n\")\n",
        "    print(\"PS. The input layer is not shown, altough you should be able to calculate it by looking at <Param #>\")\n",
        "    print(\" of the first hidden layer.\\n\")\n",
        "    mock_model.summary()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVAUHWEVzcO5"
      },
      "source": [
        "## Create an Experience Buffer\n",
        "\n",
        "For a neural network to perform optimally we want the data to be I.I.D (Independent and Identically Distributed). In supervised learning, where for instance the network is fed an image and it will predict either cat or dog, this is achieved by randomly sampling the training data from the full data set. As a result:\n",
        "- Batches have close-to similar data distribution.\n",
        "- Samples in each batch are independent of each other.\n",
        "\n",
        "In Reinforcement learning where our agent samples data by moving from state to state, the recently sampled data will be highly correlated to each other. As a result, we will feed our network data which closely resembles each other, which is a recipe for disaster when working with a neural network. Furthermore, the data distribution of the initial states will be different from that of the later stage, i.e. this does not satisfy the I.I.D criterion. This is bad news!\n",
        "\n",
        "Luckily Mnih et. al. has a solution. Instead of training on the just the previously collected samples, we store all states in an **Experience Buffer** and sample randomly from this buffer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WTdYCeMzcO5"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "This section is divided into multiple tasks. You will first complete task 1, and then run the first assertion cell for task 1. Then you will continue to task 2 and run assertion cell 2 etc.\n",
        "\n",
        "### Task 1 - Add experience\n",
        "\n",
        "The first method we are going to create is a method which adds an experience, i.e. a state-transition. This is stored as a tuple which contains the following fields; state, action, reward, done, new_state.\n",
        "\n",
        "To store a state-transition we are going to use a specific type of tuple, namely a namedtuple. You can read more about it [here](https://docs.python.org/2/library/collections.html#collections.namedtuple). We have created a namedtuple which you are going to use called experience_tuple. The experience_tuple have the following field where you can store information; state, action, reward, done, new_state. To create a state-transition do the following:\n",
        "- self.experience_tuple(state=your_state, action=your_action, reward=your_reward, done=your_done, new_state=your_new_state)\n",
        "\n",
        "To add a named_tuple to the buffer use the append method of the experience_buffer:\n",
        "- self.experience_buffer.append(your_experience_tuple)\n",
        "\n",
        "\n",
        "### Task 2 - Get batch\n",
        "\n",
        "The next method we are going to create is a method which returns a batch of experience_tuples, which is used during training. The methods should set the variable *experiences* equal to either\n",
        "- All the samples in the experience_buffer IF it is less than the batch size\n",
        "- or ELSE a *batch_size* of randomly sample experience_tuples from the experience_buffer. To randomly sample use the random.sample() method. You can read more about the method [here](https://docs.python.org/2/library/random.html).\n",
        "\n",
        "The last part of the method is created by us, and it creates vectors of all elements of each field. This is done to vectorize the training and gives a significant reduction in training time.\n",
        "\n",
        "### Task 3 - Warm-up\n",
        "\n",
        "The Experience Buffer is a collection of all the experiences the agent has collected during its interaction with the environment. As a result, the experience buffer is empty at the begin of the first episode. This is not an issue, as you in the previous task implemented the necessary logic to handle the case when there are fewer experience_touples in the buffer than the batch size. However, this does slow down training as we train with few and highly correlated samples. \n",
        "\n",
        "To counteract this issue, we introduce a method called warm_up. The warm_up method performs completes a certain amount of episodes taking random actions, thereby creating reducing the correlation between samples in the initial training phase.\n",
        "\n",
        "We have created the high-level structure of this method, but you will create the code which completes one episode. The method should do the following\n",
        "1. Use a While-loop which runs as long as the Done parameter is \"not true\"\n",
        "2. Chooses a random action - Use env.action_space.sample()\n",
        "3. Use the random action to make a step in the environment - Use env.step(random_action).\n",
        "    - env.step() returns the following variables accordingly; new_state, reward, done, state_transition_probability\n",
        "    - We will not use the state_transition_probability variable\n",
        "4. Add the experience to the replay_buffer - Use the add_experience method we created earlier\n",
        "5. Remember to update the current state, to the new_state, e.g. state = new_state\n",
        "\n",
        "Remember to test your method by running the assertion cell for task 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4adpSRr_zcO5"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay:\n",
        "    \n",
        "    def __init__(self, env, parameters):\n",
        "        self.env = env\n",
        "        self.buffer_size = parameters[\"buffer_size\"]\n",
        "        self.batch_size = parameters[\"batch_size\"]\n",
        "        self.experience_buffer = deque(maxlen=self.buffer_size)\n",
        "        \n",
        "        self.experience_tuple = namedtuple(\"Experience\", \n",
        "                                           field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"])\n",
        "    \n",
        "    def add_experience(self, state, action, reward, done, new_state):\n",
        "        \n",
        "        \"Input code below\"\n",
        "\n",
        "        \"Input code above\"\n",
        "        \n",
        "    def get_batch(self):\n",
        "        \n",
        "        \"Input code below\"\n",
        "        \n",
        "        \"Input code above\"\n",
        "        \n",
        "        states = np.vstack([e.state for e in experiences if e is not None])\n",
        "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
        "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
        "        new_states = np.vstack([e.new_state for e in experiences if e is not None])\n",
        "        dones = np.vstack([e.done for e in experiences if e is not None])\n",
        "        \n",
        "        return (states, actions, rewards, dones, new_states)\n",
        "    \n",
        "    def warm_up(self):\n",
        "        for _ in range(10):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            \n",
        "            \"Input code below\"\n",
        "\n",
        "            \"Input code above\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuCXzCC3zcO6"
      },
      "source": [
        "### Assertion - Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTur1xZazcO6",
        "outputId": "24b37783-4072-4380-dd9d-06d87f8e767b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You implemented the add_experience method correctly, great job!\n"
          ]
        }
      ],
      "source": [
        "# Do not edit - Assertion cell #\n",
        "if should_assert:\n",
        "    DummyBuffer = create_dummybuffer()\n",
        "\n",
        "    for i in range(6):\n",
        "        DummyBuffer.add_experience(i, i%2, i, i%2, i+1)\n",
        "\n",
        "    dummy_experience_buffer = DummyBuffer.experience_buffer.copy()\n",
        "    assert(len(dummy_experience_buffer) == 6),\\\n",
        "    \"Length of your experience buffer is wrong, should be 6, was {}\".format(len(dummy_experience_buffer))\n",
        "\n",
        "    for e in range(6):\n",
        "        dummy_experience = DummyBuffer.experience_tuple(state=e, action=e%2, reward=e, done=e%2, new_state=e+1)\n",
        "        stored_experience = dummy_experience_buffer.popleft()\n",
        "        assert(dummy_experience == stored_experience), \\\n",
        "        \"The values were incorrectly stored, was {}, should have been {}\".format(stored_experience, dummy_experience)\n",
        "\n",
        "    print(\"You implemented the add_experience method correctly, great job!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ_hsbYBzcO6"
      },
      "source": [
        "### Assertion - Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoV7tQKtzcO7",
        "outputId": "c1ac520c-1572-484d-bc35-4a47572f7a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great, you implemented the get_batch() method correctly\n"
          ]
        }
      ],
      "source": [
        "# Do not edit - Assertion cell #\n",
        "if should_assert:\n",
        "    DummyBuffer = create_dummybuffer()\n",
        "    for i in range(2):\n",
        "        DummyBuffer.add_experience(i, i%2, i, i%2, i+1)\n",
        "\n",
        "    assert(len(DummyBuffer.get_batch()[0]) == 2),\\\n",
        "    \"Should return 2 samples, when there are less experience_tuples than the batch_size. Your method returned {} samples\"\\\n",
        "    .format(len(DummyBuffer.get_batch()[0]))\n",
        "\n",
        "    DummyBuffer.add_experience(3, 3%2, 3, 3%2, 3+1)\n",
        "    dummy_batch = DummyBuffer.get_batch()\n",
        "    assert(len(dummy_batch[0]) == 3),\\\n",
        "    \"Should return 3 samples, when there are 3 experience_tuples in the experience_buffer. Your method returned {} samples\"\\\n",
        "    .format(len(DummyBuffer.get_batch()[0]))\n",
        "\n",
        "\n",
        "    DummyBuffer.add_experience(4, 4%2, 4, 4%2, 4+1)\n",
        "    dummy_batch = DummyBuffer.get_batch()\n",
        "    assert(len(dummy_batch[0] == 3)),\\\n",
        "    \"Should return 3 samples, when there are more experience_tuples than the batch_size. Your method returned {} samples\"\\\n",
        "    .format(len(DummyBuffer.get_batch()[0]))\n",
        "\n",
        "    assert(len(np.unique(dummy_batch[0][0:3])) == 3), \"The method did not return the values correctly, states should be all unique values. You returned {}\".format(dummy_batch[0])\n",
        "    assert(len(np.unique(dummy_batch[-1][0:3])) == 3), \"The method did not return the values correctly, new_states should be all unique values. You returned {}\".format(dummy_batch[-1])\n",
        "\n",
        "    print(\"Great, you implemented the get_batch() method correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXw-VZ-kzcO7"
      },
      "source": [
        "### Assertion Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO27XXnkzcO7",
        "outputId": "dfe4e82b-7634-4c23-c8fd-f21497cee263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0\n",
            "Experience(state=array([ 0.015, -0.006,  0.017, -0.011]), action=1, reward=1.0, done=False, new_state=array([ 0.015,  0.189,  0.016, -0.299]))\n",
            "1\n",
            "Experience(state=array([ 0.015,  0.189,  0.016, -0.299]), action=1, reward=1.0, done=False, new_state=array([ 0.019,  0.384,  0.01 , -0.586]))\n",
            "1\n",
            "Experience(state=array([ 0.019,  0.384,  0.01 , -0.586]), action=1, reward=1.0, done=False, new_state=array([ 0.027,  0.579, -0.001, -0.875]))\n",
            "1\n",
            "Experience(state=array([ 0.027,  0.579, -0.001, -0.875]), action=1, reward=1.0, done=False, new_state=array([ 0.038,  0.774, -0.019, -1.169]))\n",
            "1\n",
            "Experience(state=array([ 0.038,  0.774, -0.019, -1.169]), action=0, reward=1.0, done=False, new_state=array([ 0.054,  0.579, -0.042, -0.882]))\n",
            "1\n",
            "Experience(state=array([ 0.054,  0.579, -0.042, -0.882]), action=0, reward=1.0, done=False, new_state=array([ 0.065,  0.385, -0.06 , -0.603]))\n",
            "1\n",
            "Experience(state=array([ 0.065,  0.385, -0.06 , -0.603]), action=0, reward=1.0, done=False, new_state=array([ 0.073,  0.191, -0.072, -0.329]))\n",
            "0\n",
            "Experience(state=array([ 0.073,  0.191, -0.072, -0.329]), action=0, reward=1.0, done=False, new_state=array([ 0.077, -0.004, -0.078, -0.06 ]))\n",
            "1\n",
            "Experience(state=array([ 0.077, -0.004, -0.078, -0.06 ]), action=0, reward=1.0, done=False, new_state=array([ 0.077, -0.197, -0.08 ,  0.207]))\n",
            "1\n",
            "Experience(state=array([ 0.077, -0.197, -0.08 ,  0.207]), action=1, reward=1.0, done=False, new_state=array([ 0.073, -0.001, -0.075, -0.11 ]))\n",
            "1\n",
            "Experience(state=array([ 0.073, -0.001, -0.075, -0.11 ]), action=1, reward=1.0, done=False, new_state=array([ 0.073,  0.195, -0.078, -0.425]))\n",
            "0\n",
            "Experience(state=array([ 0.073,  0.195, -0.078, -0.425]), action=1, reward=1.0, done=False, new_state=array([ 0.077,  0.391, -0.086, -0.741]))\n",
            "1\n",
            "Experience(state=array([ 0.077,  0.391, -0.086, -0.741]), action=0, reward=1.0, done=False, new_state=array([ 0.084,  0.197, -0.101, -0.477]))\n",
            "0\n",
            "Experience(state=array([ 0.084,  0.197, -0.101, -0.477]), action=0, reward=1.0, done=False, new_state=array([ 0.088,  0.004, -0.111, -0.218]))\n",
            "0\n",
            "Experience(state=array([ 0.088,  0.004, -0.111, -0.218]), action=0, reward=1.0, done=False, new_state=array([ 0.088, -0.19 , -0.115,  0.038]))\n",
            "0\n",
            "Experience(state=array([ 0.088, -0.19 , -0.115,  0.038]), action=0, reward=1.0, done=False, new_state=array([ 0.085, -0.383, -0.114,  0.292]))\n",
            "1\n",
            "Experience(state=array([ 0.085, -0.383, -0.114,  0.292]), action=1, reward=1.0, done=False, new_state=array([ 0.077, -0.187, -0.108, -0.034]))\n",
            "1\n",
            "Experience(state=array([ 0.077, -0.187, -0.108, -0.034]), action=0, reward=1.0, done=False, new_state=array([ 0.073, -0.38 , -0.109,  0.223]))\n",
            "0\n",
            "Experience(state=array([ 0.073, -0.38 , -0.109,  0.223]), action=0, reward=1.0, done=False, new_state=array([ 0.066, -0.573, -0.105,  0.479]))\n",
            "0\n",
            "Experience(state=array([ 0.066, -0.573, -0.105,  0.479]), action=1, reward=1.0, done=False, new_state=array([ 0.054, -0.377, -0.095,  0.155]))\n",
            "1\n",
            "Experience(state=array([ 0.054, -0.377, -0.095,  0.155]), action=1, reward=1.0, done=False, new_state=array([ 0.047, -0.181, -0.092, -0.166]))\n",
            "1\n",
            "Experience(state=array([ 0.047, -0.181, -0.092, -0.166]), action=1, reward=1.0, done=False, new_state=array([ 0.043,  0.016, -0.095, -0.486]))\n",
            "0\n",
            "Experience(state=array([ 0.043,  0.016, -0.095, -0.486]), action=0, reward=1.0, done=False, new_state=array([ 0.043, -0.178, -0.105, -0.225]))\n",
            "1\n",
            "Experience(state=array([ 0.043, -0.178, -0.105, -0.225]), action=1, reward=1.0, done=False, new_state=array([ 0.04 ,  0.019, -0.109, -0.549]))\n",
            "1\n",
            "Experience(state=array([ 0.04 ,  0.019, -0.109, -0.549]), action=0, reward=1.0, done=False, new_state=array([ 0.04 , -0.175, -0.12 , -0.292]))\n",
            "1\n",
            "Experience(state=array([ 0.04 , -0.175, -0.12 , -0.292]), action=1, reward=1.0, done=False, new_state=array([ 0.037,  0.022, -0.126, -0.62 ]))\n",
            "1\n",
            "Experience(state=array([ 0.037,  0.022, -0.126, -0.62 ]), action=0, reward=1.0, done=False, new_state=array([ 0.037, -0.171, -0.139, -0.37 ]))\n",
            "1\n",
            "Experience(state=array([ 0.037, -0.171, -0.139, -0.37 ]), action=1, reward=1.0, done=False, new_state=array([ 0.034,  0.025, -0.146, -0.703]))\n",
            "0\n",
            "Experience(state=array([ 0.034,  0.025, -0.146, -0.703]), action=1, reward=1.0, done=False, new_state=array([ 0.034,  0.222, -0.16 , -1.038]))\n",
            "1\n",
            "Experience(state=array([ 0.034,  0.222, -0.16 , -1.038]), action=1, reward=1.0, done=False, new_state=array([ 0.039,  0.419, -0.181, -1.376]))\n",
            "1\n",
            "Experience(state=array([ 0.039,  0.419, -0.181, -1.376]), action=1, reward=1.0, done=False, new_state=array([ 0.047,  0.616, -0.208, -1.719]))\n",
            "0\n",
            "Experience(state=array([ 0.047,  0.616, -0.208, -1.719]), action=0, reward=1.0, done=True, new_state=array([ 0.059,  0.424, -0.243, -1.498]))\n",
            "0\n",
            "Experience(state=array([ 0.036,  0.045,  0.003, -0.038]), action=0, reward=1.0, done=False, new_state=array([ 0.036, -0.15 ,  0.002,  0.255]))\n",
            "1\n",
            "Experience(state=array([ 0.036, -0.15 ,  0.002,  0.255]), action=0, reward=1.0, done=False, new_state=array([ 0.033, -0.345,  0.008,  0.549]))\n",
            "1\n",
            "Experience(state=array([ 0.033, -0.345,  0.008,  0.549]), action=0, reward=1.0, done=False, new_state=array([ 0.027, -0.541,  0.018,  0.844]))\n",
            "1\n",
            "Experience(state=array([ 0.027, -0.541,  0.018,  0.844]), action=1, reward=1.0, done=False, new_state=array([ 0.016, -0.346,  0.035,  0.557]))\n",
            "1\n",
            "Experience(state=array([ 0.016, -0.346,  0.035,  0.557]), action=1, reward=1.0, done=False, new_state=array([ 0.009, -0.151,  0.047,  0.276]))\n",
            "1\n",
            "Experience(state=array([ 0.009, -0.151,  0.047,  0.276]), action=1, reward=1.0, done=False, new_state=array([ 0.006,  0.043,  0.052, -0.002]))\n",
            "1\n",
            "Experience(state=array([ 0.006,  0.043,  0.052, -0.002]), action=0, reward=1.0, done=False, new_state=array([ 0.007, -0.153,  0.052,  0.307]))\n",
            "0\n",
            "Experience(state=array([ 0.007, -0.153,  0.052,  0.307]), action=0, reward=1.0, done=False, new_state=array([ 0.004, -0.348,  0.058,  0.615]))\n",
            "0\n",
            "Experience(state=array([ 0.004, -0.348,  0.058,  0.615]), action=0, reward=1.0, done=False, new_state=array([-0.003, -0.544,  0.07 ,  0.926]))\n",
            "1\n",
            "Experience(state=array([-0.003, -0.544,  0.07 ,  0.926]), action=1, reward=1.0, done=False, new_state=array([-0.014, -0.35 ,  0.089,  0.656]))\n",
            "0\n",
            "Experience(state=array([-0.014, -0.35 ,  0.089,  0.656]), action=1, reward=1.0, done=False, new_state=array([-0.021, -0.156,  0.102,  0.393]))\n",
            "1\n",
            "Experience(state=array([-0.021, -0.156,  0.102,  0.393]), action=1, reward=1.0, done=False, new_state=array([-0.024,  0.037,  0.11 ,  0.134]))\n",
            "0\n",
            "Experience(state=array([-0.024,  0.037,  0.11 ,  0.134]), action=1, reward=1.0, done=False, new_state=array([-0.024,  0.23 ,  0.113, -0.122]))\n",
            "0\n",
            "Experience(state=array([-0.024,  0.23 ,  0.113, -0.122]), action=1, reward=1.0, done=False, new_state=array([-0.019,  0.424,  0.11 , -0.377]))\n",
            "0\n",
            "Experience(state=array([-0.019,  0.424,  0.11 , -0.377]), action=1, reward=1.0, done=False, new_state=array([-0.011,  0.617,  0.103, -0.633]))\n",
            "0\n",
            "Experience(state=array([-0.011,  0.617,  0.103, -0.633]), action=0, reward=1.0, done=False, new_state=array([ 0.002,  0.421,  0.09 , -0.31 ]))\n",
            "0\n",
            "Experience(state=array([ 0.002,  0.421,  0.09 , -0.31 ]), action=0, reward=1.0, done=False, new_state=array([0.01 , 0.225, 0.084, 0.009]))\n",
            "1\n",
            "Experience(state=array([0.01 , 0.225, 0.084, 0.009]), action=0, reward=1.0, done=False, new_state=array([0.015, 0.028, 0.084, 0.327]))\n",
            "0\n",
            "Experience(state=array([0.015, 0.028, 0.084, 0.327]), action=0, reward=1.0, done=False, new_state=array([ 0.015, -0.168,  0.09 ,  0.645]))\n",
            "0\n",
            "Experience(state=array([ 0.015, -0.168,  0.09 ,  0.645]), action=1, reward=1.0, done=False, new_state=array([0.012, 0.026, 0.103, 0.382]))\n",
            "1\n",
            "Experience(state=array([0.012, 0.026, 0.103, 0.382]), action=1, reward=1.0, done=False, new_state=array([0.012, 0.219, 0.111, 0.124]))\n",
            "0\n",
            "Experience(state=array([0.012, 0.219, 0.111, 0.124]), action=1, reward=1.0, done=False, new_state=array([ 0.017,  0.413,  0.114, -0.132]))\n",
            "0\n",
            "Experience(state=array([ 0.017,  0.413,  0.114, -0.132]), action=0, reward=1.0, done=False, new_state=array([0.025, 0.216, 0.111, 0.195]))\n",
            "1\n",
            "Experience(state=array([0.025, 0.216, 0.111, 0.195]), action=1, reward=1.0, done=False, new_state=array([ 0.029,  0.41 ,  0.115, -0.061]))\n",
            "1\n",
            "Experience(state=array([ 0.029,  0.41 ,  0.115, -0.061]), action=0, reward=1.0, done=False, new_state=array([0.038, 0.213, 0.114, 0.265]))\n",
            "1\n",
            "Experience(state=array([0.038, 0.213, 0.114, 0.265]), action=1, reward=1.0, done=False, new_state=array([0.042, 0.406, 0.119, 0.011]))\n",
            "1\n",
            "Experience(state=array([0.042, 0.406, 0.119, 0.011]), action=0, reward=1.0, done=False, new_state=array([0.05 , 0.21 , 0.119, 0.338]))\n",
            "0\n",
            "Experience(state=array([0.05 , 0.21 , 0.119, 0.338]), action=1, reward=1.0, done=False, new_state=array([0.054, 0.403, 0.126, 0.085]))\n",
            "1\n",
            "Experience(state=array([0.054, 0.403, 0.126, 0.085]), action=0, reward=1.0, done=False, new_state=array([0.062, 0.206, 0.128, 0.415]))\n",
            "1\n",
            "Experience(state=array([0.062, 0.206, 0.128, 0.415]), action=0, reward=1.0, done=False, new_state=array([0.066, 0.01 , 0.136, 0.745]))\n",
            "0\n",
            "Experience(state=array([0.066, 0.01 , 0.136, 0.745]), action=0, reward=1.0, done=False, new_state=array([ 0.067, -0.187,  0.151,  1.077]))\n",
            "1\n",
            "Experience(state=array([ 0.067, -0.187,  0.151,  1.077]), action=1, reward=1.0, done=False, new_state=array([0.063, 0.006, 0.172, 0.835]))\n",
            "0\n",
            "Experience(state=array([0.063, 0.006, 0.172, 0.835]), action=0, reward=1.0, done=False, new_state=array([ 0.063, -0.191,  0.189,  1.177]))\n",
            "0\n",
            "Experience(state=array([ 0.063, -0.191,  0.189,  1.177]), action=1, reward=1.0, done=True, new_state=array([0.059, 0.001, 0.213, 0.949]))\n",
            "1\n",
            "Experience(state=array([-0.002,  0.008,  0.038,  0.001]), action=1, reward=1.0, done=False, new_state=array([-0.001,  0.202,  0.038, -0.279]))\n",
            "0\n",
            "Experience(state=array([-0.001,  0.202,  0.038, -0.279]), action=0, reward=1.0, done=False, new_state=array([0.003, 0.007, 0.032, 0.025]))\n",
            "0\n",
            "Experience(state=array([0.003, 0.007, 0.032, 0.025]), action=0, reward=1.0, done=False, new_state=array([ 0.003, -0.189,  0.033,  0.328]))\n",
            "1\n",
            "Experience(state=array([ 0.003, -0.189,  0.033,  0.328]), action=1, reward=1.0, done=False, new_state=array([-0.001,  0.006,  0.039,  0.046]))\n",
            "0\n",
            "Experience(state=array([-0.001,  0.006,  0.039,  0.046]), action=0, reward=1.0, done=False, new_state=array([-0.001, -0.19 ,  0.04 ,  0.35 ]))\n",
            "1\n",
            "Experience(state=array([-0.001, -0.19 ,  0.04 ,  0.35 ]), action=0, reward=1.0, done=False, new_state=array([-0.005, -0.385,  0.047,  0.655]))\n",
            "1\n",
            "Experience(state=array([-0.005, -0.385,  0.047,  0.655]), action=0, reward=1.0, done=False, new_state=array([-0.012, -0.581,  0.06 ,  0.963]))\n",
            "0\n",
            "Experience(state=array([-0.012, -0.581,  0.06 ,  0.963]), action=1, reward=1.0, done=False, new_state=array([-0.024, -0.387,  0.08 ,  0.689]))\n",
            "0\n",
            "Experience(state=array([-0.024, -0.387,  0.08 ,  0.689]), action=0, reward=1.0, done=False, new_state=array([-0.032, -0.583,  0.093,  1.006]))\n",
            "0\n",
            "Experience(state=array([-0.032, -0.583,  0.093,  1.006]), action=0, reward=1.0, done=False, new_state=array([-0.043, -0.779,  0.113,  1.327]))\n",
            "1\n",
            "Experience(state=array([-0.043, -0.779,  0.113,  1.327]), action=1, reward=1.0, done=False, new_state=array([-0.059, -0.586,  0.14 ,  1.071]))\n",
            "1\n",
            "Experience(state=array([-0.059, -0.586,  0.14 ,  1.071]), action=0, reward=1.0, done=False, new_state=array([-0.071, -0.782,  0.161,  1.405]))\n",
            "1\n",
            "Experience(state=array([-0.071, -0.782,  0.161,  1.405]), action=1, reward=1.0, done=False, new_state=array([-0.086, -0.59 ,  0.189,  1.166]))\n",
            "0\n",
            "Experience(state=array([-0.086, -0.59 ,  0.189,  1.166]), action=1, reward=1.0, done=True, new_state=array([-0.098, -0.397,  0.213,  0.939]))\n",
            "0\n",
            "Experience(state=array([-0.012,  0.004, -0.008,  0.023]), action=0, reward=1.0, done=False, new_state=array([-0.011, -0.191, -0.008,  0.313]))\n",
            "0\n",
            "Experience(state=array([-0.011, -0.191, -0.008,  0.313]), action=1, reward=1.0, done=False, new_state=array([-0.015,  0.004, -0.002,  0.018]))\n",
            "1\n",
            "Experience(state=array([-0.015,  0.004, -0.002,  0.018]), action=1, reward=1.0, done=False, new_state=array([-0.015,  0.199, -0.001, -0.276]))\n",
            "0\n",
            "Experience(state=array([-0.015,  0.199, -0.001, -0.276]), action=0, reward=1.0, done=False, new_state=array([-0.011,  0.004, -0.007,  0.017]))\n",
            "0\n",
            "Experience(state=array([-0.011,  0.004, -0.007,  0.017]), action=1, reward=1.0, done=False, new_state=array([-0.011,  0.199, -0.006, -0.278]))\n",
            "0\n",
            "Experience(state=array([-0.011,  0.199, -0.006, -0.278]), action=0, reward=1.0, done=False, new_state=array([-0.007,  0.004, -0.012,  0.013]))\n",
            "1\n",
            "Experience(state=array([-0.007,  0.004, -0.012,  0.013]), action=1, reward=1.0, done=False, new_state=array([-0.007,  0.2  , -0.012, -0.284]))\n",
            "0\n",
            "Experience(state=array([-0.007,  0.2  , -0.012, -0.284]), action=0, reward=1.0, done=False, new_state=array([-0.003,  0.005, -0.017,  0.005]))\n",
            "0\n",
            "Experience(state=array([-0.003,  0.005, -0.017,  0.005]), action=0, reward=1.0, done=False, new_state=array([-0.003, -0.19 , -0.017,  0.292]))\n",
            "1\n",
            "Experience(state=array([-0.003, -0.19 , -0.017,  0.292]), action=1, reward=1.0, done=False, new_state=array([-0.007,  0.005, -0.011, -0.006]))\n",
            "0\n",
            "Experience(state=array([-0.007,  0.005, -0.011, -0.006]), action=1, reward=1.0, done=False, new_state=array([-0.007,  0.2  , -0.012, -0.302]))\n",
            "1\n",
            "Experience(state=array([-0.007,  0.2  , -0.012, -0.302]), action=1, reward=1.0, done=False, new_state=array([-0.003,  0.396, -0.018, -0.598]))\n",
            "0\n",
            "Experience(state=array([-0.003,  0.396, -0.018, -0.598]), action=1, reward=1.0, done=False, new_state=array([ 0.005,  0.591, -0.03 , -0.897]))\n",
            "0\n",
            "Experience(state=array([ 0.005,  0.591, -0.03 , -0.897]), action=1, reward=1.0, done=False, new_state=array([ 0.017,  0.787, -0.048, -1.198]))\n",
            "1\n",
            "Experience(state=array([ 0.017,  0.787, -0.048, -1.198]), action=0, reward=1.0, done=False, new_state=array([ 0.033,  0.592, -0.071, -0.921]))\n",
            "1\n",
            "Experience(state=array([ 0.033,  0.592, -0.071, -0.921]), action=0, reward=1.0, done=False, new_state=array([ 0.045,  0.398, -0.09 , -0.652]))\n",
            "1\n",
            "Experience(state=array([ 0.045,  0.398, -0.09 , -0.652]), action=0, reward=1.0, done=False, new_state=array([ 0.053,  0.204, -0.103, -0.389]))\n",
            "0\n",
            "Experience(state=array([ 0.053,  0.204, -0.103, -0.389]), action=0, reward=1.0, done=False, new_state=array([ 0.057,  0.011, -0.111, -0.13 ]))\n",
            "1\n",
            "Experience(state=array([ 0.057,  0.011, -0.111, -0.13 ]), action=1, reward=1.0, done=False, new_state=array([ 0.057,  0.207, -0.113, -0.456]))\n",
            "0\n",
            "Experience(state=array([ 0.057,  0.207, -0.113, -0.456]), action=0, reward=1.0, done=False, new_state=array([ 0.061,  0.014, -0.122, -0.201]))\n",
            "1\n",
            "Experience(state=array([ 0.061,  0.014, -0.122, -0.201]), action=1, reward=1.0, done=False, new_state=array([ 0.061,  0.211, -0.126, -0.529]))\n",
            "1\n",
            "Experience(state=array([ 0.061,  0.211, -0.126, -0.529]), action=1, reward=1.0, done=False, new_state=array([ 0.066,  0.407, -0.137, -0.859]))\n",
            "0\n",
            "Experience(state=array([ 0.066,  0.407, -0.137, -0.859]), action=0, reward=1.0, done=False, new_state=array([ 0.074,  0.214, -0.154, -0.612]))\n",
            "1\n",
            "Experience(state=array([ 0.074,  0.214, -0.154, -0.612]), action=1, reward=1.0, done=False, new_state=array([ 0.078,  0.411, -0.166, -0.949]))\n",
            "0\n",
            "Experience(state=array([ 0.078,  0.411, -0.166, -0.949]), action=0, reward=1.0, done=False, new_state=array([ 0.086,  0.219, -0.185, -0.713]))\n",
            "0\n",
            "Experience(state=array([ 0.086,  0.219, -0.185, -0.713]), action=1, reward=1.0, done=False, new_state=array([ 0.091,  0.416, -0.2  , -1.058]))\n",
            "1\n",
            "Experience(state=array([ 0.091,  0.416, -0.2  , -1.058]), action=1, reward=1.0, done=True, new_state=array([ 0.099,  0.613, -0.221, -1.406]))\n",
            "0\n",
            "Experience(state=array([-0.018,  0.025,  0.02 , -0.016]), action=0, reward=1.0, done=False, new_state=array([-0.018, -0.17 ,  0.02 ,  0.283]))\n",
            "1\n",
            "Experience(state=array([-0.018, -0.17 ,  0.02 ,  0.283]), action=0, reward=1.0, done=False, new_state=array([-0.021, -0.366,  0.025,  0.581]))\n",
            "0\n",
            "Experience(state=array([-0.021, -0.366,  0.025,  0.581]), action=0, reward=1.0, done=False, new_state=array([-0.029, -0.561,  0.037,  0.882]))\n",
            "0\n",
            "Experience(state=array([-0.029, -0.561,  0.037,  0.882]), action=1, reward=1.0, done=False, new_state=array([-0.04 , -0.366,  0.055,  0.601]))\n",
            "1\n",
            "Experience(state=array([-0.04 , -0.366,  0.055,  0.601]), action=1, reward=1.0, done=False, new_state=array([-0.047, -0.172,  0.067,  0.326]))\n",
            "1\n",
            "Experience(state=array([-0.047, -0.172,  0.067,  0.326]), action=1, reward=1.0, done=False, new_state=array([-0.051,  0.022,  0.073,  0.055]))\n",
            "1\n",
            "Experience(state=array([-0.051,  0.022,  0.073,  0.055]), action=1, reward=1.0, done=False, new_state=array([-0.05 ,  0.216,  0.074, -0.213]))\n",
            "0\n",
            "Experience(state=array([-0.05 ,  0.216,  0.074, -0.213]), action=0, reward=1.0, done=False, new_state=array([-0.046,  0.02 ,  0.07 ,  0.102]))\n",
            "0\n",
            "Experience(state=array([-0.046,  0.02 ,  0.07 ,  0.102]), action=1, reward=1.0, done=False, new_state=array([-0.045,  0.214,  0.072, -0.168]))\n",
            "1\n",
            "Experience(state=array([-0.045,  0.214,  0.072, -0.168]), action=0, reward=1.0, done=False, new_state=array([-0.041,  0.018,  0.069,  0.147]))\n",
            "0\n",
            "Experience(state=array([-0.041,  0.018,  0.069,  0.147]), action=1, reward=1.0, done=False, new_state=array([-0.041,  0.212,  0.072, -0.124]))\n",
            "0\n",
            "Experience(state=array([-0.041,  0.212,  0.072, -0.124]), action=0, reward=1.0, done=False, new_state=array([-0.037,  0.016,  0.069,  0.191]))\n",
            "1\n",
            "Experience(state=array([-0.037,  0.016,  0.069,  0.191]), action=1, reward=1.0, done=False, new_state=array([-0.036,  0.21 ,  0.073, -0.079]))\n",
            "1\n",
            "Experience(state=array([-0.036,  0.21 ,  0.073, -0.079]), action=1, reward=1.0, done=False, new_state=array([-0.032,  0.404,  0.071, -0.348]))\n",
            "0\n",
            "Experience(state=array([-0.032,  0.404,  0.071, -0.348]), action=0, reward=1.0, done=False, new_state=array([-0.024,  0.208,  0.065, -0.034]))\n",
            "0\n",
            "Experience(state=array([-0.024,  0.208,  0.065, -0.034]), action=1, reward=1.0, done=False, new_state=array([-0.02 ,  0.402,  0.064, -0.305]))\n",
            "0\n",
            "Experience(state=array([-0.02 ,  0.402,  0.064, -0.305]), action=1, reward=1.0, done=False, new_state=array([-0.012,  0.596,  0.058, -0.577]))\n",
            "1\n",
            "Experience(state=array([-0.012,  0.596,  0.058, -0.577]), action=1, reward=1.0, done=False, new_state=array([ 1.887e-04,  7.904e-01,  4.618e-02, -8.512e-01]))\n",
            "1\n",
            "Experience(state=array([ 1.887e-04,  7.904e-01,  4.618e-02, -8.512e-01]), action=1, reward=1.0, done=False, new_state=array([ 0.016,  0.985,  0.029, -1.129]))\n",
            "1\n",
            "Experience(state=array([ 0.016,  0.985,  0.029, -1.129]), action=1, reward=1.0, done=False, new_state=array([ 0.036,  1.18 ,  0.007, -1.412]))\n",
            "0\n",
            "Experience(state=array([ 0.036,  1.18 ,  0.007, -1.412]), action=0, reward=1.0, done=False, new_state=array([ 0.059,  0.984, -0.022, -1.118]))\n",
            "0\n",
            "Experience(state=array([ 0.059,  0.984, -0.022, -1.118]), action=0, reward=1.0, done=False, new_state=array([ 0.079,  0.79 , -0.044, -0.832]))\n",
            "1\n",
            "Experience(state=array([ 0.079,  0.79 , -0.044, -0.832]), action=1, reward=1.0, done=False, new_state=array([ 0.095,  0.985, -0.061, -1.138]))\n",
            "0\n",
            "Experience(state=array([ 0.095,  0.985, -0.061, -1.138]), action=1, reward=1.0, done=False, new_state=array([ 0.114,  1.181, -0.083, -1.449]))\n",
            "0\n",
            "Experience(state=array([ 0.114,  1.181, -0.083, -1.449]), action=1, reward=1.0, done=False, new_state=array([ 0.138,  1.377, -0.112, -1.767]))\n",
            "1\n",
            "Experience(state=array([ 0.138,  1.377, -0.112, -1.767]), action=0, reward=1.0, done=False, new_state=array([ 0.166,  1.183, -0.148, -1.511]))\n",
            "0\n",
            "Experience(state=array([ 0.166,  1.183, -0.148, -1.511]), action=1, reward=1.0, done=False, new_state=array([ 0.189,  1.38 , -0.178, -1.846]))\n",
            "0\n",
            "Experience(state=array([ 0.189,  1.38 , -0.178, -1.846]), action=1, reward=1.0, done=True, new_state=array([ 0.217,  1.577, -0.215, -2.188]))\n",
            "0\n",
            "Experience(state=array([ 0.022,  0.02 , -0.03 , -0.021]), action=1, reward=1.0, done=False, new_state=array([ 0.023,  0.216, -0.031, -0.323]))\n",
            "1\n",
            "Experience(state=array([ 0.023,  0.216, -0.031, -0.323]), action=1, reward=1.0, done=False, new_state=array([ 0.027,  0.412, -0.037, -0.625]))\n",
            "1\n",
            "Experience(state=array([ 0.027,  0.412, -0.037, -0.625]), action=1, reward=1.0, done=False, new_state=array([ 0.035,  0.607, -0.05 , -0.929]))\n",
            "0\n",
            "Experience(state=array([ 0.035,  0.607, -0.05 , -0.929]), action=0, reward=1.0, done=False, new_state=array([ 0.048,  0.413, -0.068, -0.653]))\n",
            "0\n",
            "Experience(state=array([ 0.048,  0.413, -0.068, -0.653]), action=0, reward=1.0, done=False, new_state=array([ 0.056,  0.219, -0.081, -0.382]))\n",
            "0\n",
            "Experience(state=array([ 0.056,  0.219, -0.081, -0.382]), action=1, reward=1.0, done=False, new_state=array([ 0.06 ,  0.415, -0.089, -0.699]))\n",
            "0\n",
            "Experience(state=array([ 0.06 ,  0.415, -0.089, -0.699]), action=1, reward=1.0, done=False, new_state=array([ 0.068,  0.611, -0.103, -1.019]))\n",
            "1\n",
            "Experience(state=array([ 0.068,  0.611, -0.103, -1.019]), action=0, reward=1.0, done=False, new_state=array([ 0.081,  0.417, -0.123, -0.76 ]))\n",
            "1\n",
            "Experience(state=array([ 0.081,  0.417, -0.123, -0.76 ]), action=1, reward=1.0, done=False, new_state=array([ 0.089,  0.614, -0.139, -1.089]))\n",
            "1\n",
            "Experience(state=array([ 0.089,  0.614, -0.139, -1.089]), action=1, reward=1.0, done=False, new_state=array([ 0.101,  0.811, -0.16 , -1.422]))\n",
            "0\n",
            "Experience(state=array([ 0.101,  0.811, -0.16 , -1.422]), action=0, reward=1.0, done=False, new_state=array([ 0.118,  0.618, -0.189, -1.183]))\n",
            "1\n",
            "Experience(state=array([ 0.118,  0.618, -0.189, -1.183]), action=1, reward=1.0, done=True, new_state=array([ 0.13 ,  0.815, -0.212, -1.528]))\n",
            "1\n",
            "Experience(state=array([ 0.049, -0.035,  0.027,  0.03 ]), action=0, reward=1.0, done=False, new_state=array([ 0.048, -0.231,  0.027,  0.331]))\n",
            "1\n",
            "Experience(state=array([ 0.048, -0.231,  0.027,  0.331]), action=0, reward=1.0, done=False, new_state=array([ 0.044, -0.426,  0.034,  0.632]))\n",
            "1\n",
            "Experience(state=array([ 0.044, -0.426,  0.034,  0.632]), action=1, reward=1.0, done=False, new_state=array([ 0.035, -0.232,  0.047,  0.35 ]))\n",
            "0\n",
            "Experience(state=array([ 0.035, -0.232,  0.047,  0.35 ]), action=1, reward=1.0, done=False, new_state=array([ 0.031, -0.037,  0.054,  0.073]))\n",
            "0\n",
            "Experience(state=array([ 0.031, -0.037,  0.054,  0.073]), action=1, reward=1.0, done=False, new_state=array([ 0.03 ,  0.157,  0.055, -0.203]))\n",
            "1\n",
            "Experience(state=array([ 0.03 ,  0.157,  0.055, -0.203]), action=1, reward=1.0, done=False, new_state=array([ 0.033,  0.351,  0.051, -0.477]))\n",
            "1\n",
            "Experience(state=array([ 0.033,  0.351,  0.051, -0.477]), action=1, reward=1.0, done=False, new_state=array([ 0.04 ,  0.546,  0.041, -0.754]))\n",
            "0\n",
            "Experience(state=array([ 0.04 ,  0.546,  0.041, -0.754]), action=1, reward=1.0, done=False, new_state=array([ 0.051,  0.74 ,  0.026, -1.033]))\n",
            "1\n",
            "Experience(state=array([ 0.051,  0.74 ,  0.026, -1.033]), action=1, reward=1.0, done=False, new_state=array([ 0.066,  0.935,  0.006, -1.317]))\n",
            "1\n",
            "Experience(state=array([ 0.066,  0.935,  0.006, -1.317]), action=0, reward=1.0, done=False, new_state=array([ 0.085,  0.74 , -0.021, -1.023]))\n",
            "0\n",
            "Experience(state=array([ 0.085,  0.74 , -0.021, -1.023]), action=1, reward=1.0, done=False, new_state=array([ 0.099,  0.935, -0.041, -1.322]))\n",
            "1\n",
            "Experience(state=array([ 0.099,  0.935, -0.041, -1.322]), action=1, reward=1.0, done=False, new_state=array([ 0.118,  1.131, -0.068, -1.627]))\n",
            "0\n",
            "Experience(state=array([ 0.118,  1.131, -0.068, -1.627]), action=1, reward=1.0, done=False, new_state=array([ 0.141,  1.327, -0.1  , -1.94 ]))\n",
            "1\n",
            "Experience(state=array([ 0.141,  1.327, -0.1  , -1.94 ]), action=1, reward=1.0, done=False, new_state=array([ 0.167,  1.523, -0.139, -2.262]))\n",
            "1\n",
            "Experience(state=array([ 0.167,  1.523, -0.139, -2.262]), action=1, reward=1.0, done=False, new_state=array([ 0.198,  1.719, -0.184, -2.594]))\n",
            "1\n",
            "Experience(state=array([ 0.198,  1.719, -0.184, -2.594]), action=0, reward=1.0, done=True, new_state=array([ 0.232,  1.525, -0.236, -2.363]))\n",
            "0\n",
            "Experience(state=array([-0.046,  0.043,  0.045,  0.02 ]), action=1, reward=1.0, done=False, new_state=array([-0.045,  0.237,  0.045, -0.258]))\n",
            "1\n",
            "Experience(state=array([-0.045,  0.237,  0.045, -0.258]), action=0, reward=1.0, done=False, new_state=array([-0.04 ,  0.042,  0.04 ,  0.048]))\n",
            "0\n",
            "Experience(state=array([-0.04 ,  0.042,  0.04 ,  0.048]), action=1, reward=1.0, done=False, new_state=array([-0.039,  0.236,  0.041, -0.231]))\n",
            "0\n",
            "Experience(state=array([-0.039,  0.236,  0.041, -0.231]), action=0, reward=1.0, done=False, new_state=array([-0.035,  0.04 ,  0.036,  0.074]))\n",
            "0\n",
            "Experience(state=array([-0.035,  0.04 ,  0.036,  0.074]), action=0, reward=1.0, done=False, new_state=array([-0.034, -0.155,  0.038,  0.378]))\n",
            "1\n",
            "Experience(state=array([-0.034, -0.155,  0.038,  0.378]), action=0, reward=1.0, done=False, new_state=array([-0.037, -0.351,  0.045,  0.682]))\n",
            "0\n",
            "Experience(state=array([-0.037, -0.351,  0.045,  0.682]), action=1, reward=1.0, done=False, new_state=array([-0.044, -0.156,  0.059,  0.404]))\n",
            "0\n",
            "Experience(state=array([-0.044, -0.156,  0.059,  0.404]), action=0, reward=1.0, done=False, new_state=array([-0.047, -0.352,  0.067,  0.715]))\n",
            "0\n",
            "Experience(state=array([-0.047, -0.352,  0.067,  0.715]), action=1, reward=1.0, done=False, new_state=array([-0.054, -0.158,  0.081,  0.444]))\n",
            "1\n",
            "Experience(state=array([-0.054, -0.158,  0.081,  0.444]), action=0, reward=1.0, done=False, new_state=array([-0.057, -0.354,  0.09 ,  0.761]))\n",
            "1\n",
            "Experience(state=array([-0.057, -0.354,  0.09 ,  0.761]), action=1, reward=1.0, done=False, new_state=array([-0.064, -0.16 ,  0.106,  0.498]))\n",
            "1\n",
            "Experience(state=array([-0.064, -0.16 ,  0.106,  0.498]), action=1, reward=1.0, done=False, new_state=array([-0.068,  0.033,  0.115,  0.241]))\n",
            "0\n",
            "Experience(state=array([-0.068,  0.033,  0.115,  0.241]), action=1, reward=1.0, done=False, new_state=array([-0.067,  0.226,  0.12 , -0.014]))\n",
            "1\n",
            "Experience(state=array([-0.067,  0.226,  0.12 , -0.014]), action=0, reward=1.0, done=False, new_state=array([-0.062,  0.03 ,  0.12 ,  0.315]))\n",
            "0\n",
            "Experience(state=array([-0.062,  0.03 ,  0.12 ,  0.315]), action=0, reward=1.0, done=False, new_state=array([-0.062, -0.167,  0.126,  0.643]))\n",
            "1\n",
            "Experience(state=array([-0.062, -0.167,  0.126,  0.643]), action=0, reward=1.0, done=False, new_state=array([-0.065, -0.364,  0.139,  0.972]))\n",
            "0\n",
            "Experience(state=array([-0.065, -0.364,  0.139,  0.972]), action=1, reward=1.0, done=False, new_state=array([-0.072, -0.171,  0.159,  0.726]))\n",
            "1\n",
            "Experience(state=array([-0.072, -0.171,  0.159,  0.726]), action=1, reward=1.0, done=False, new_state=array([-0.076,  0.022,  0.173,  0.487]))\n",
            "1\n",
            "Experience(state=array([-0.076,  0.022,  0.173,  0.487]), action=0, reward=1.0, done=False, new_state=array([-0.075, -0.175,  0.183,  0.829]))\n",
            "0\n",
            "Experience(state=array([-0.075, -0.175,  0.183,  0.829]), action=0, reward=1.0, done=False, new_state=array([-0.079, -0.372,  0.199,  1.173]))\n",
            "1\n",
            "Experience(state=array([-0.079, -0.372,  0.199,  1.173]), action=0, reward=1.0, done=True, new_state=array([-0.086, -0.569,  0.223,  1.521]))\n",
            "1\n",
            "Experience(state=array([-0.018, -0.006,  0.037, -0.002]), action=0, reward=1.0, done=False, new_state=array([-0.018, -0.201,  0.037,  0.302]))\n",
            "0\n",
            "Experience(state=array([-0.018, -0.201,  0.037,  0.302]), action=0, reward=1.0, done=False, new_state=array([-0.022, -0.397,  0.043,  0.606]))\n",
            "1\n",
            "Experience(state=array([-0.022, -0.397,  0.043,  0.606]), action=0, reward=1.0, done=False, new_state=array([-0.03 , -0.593,  0.055,  0.912]))\n",
            "0\n",
            "Experience(state=array([-0.03 , -0.593,  0.055,  0.912]), action=0, reward=1.0, done=False, new_state=array([-0.042, -0.788,  0.073,  1.222]))\n",
            "1\n",
            "Experience(state=array([-0.042, -0.788,  0.073,  1.222]), action=1, reward=1.0, done=False, new_state=array([-0.057, -0.594,  0.098,  0.953]))\n",
            "1\n",
            "Experience(state=array([-0.057, -0.594,  0.098,  0.953]), action=0, reward=1.0, done=False, new_state=array([-0.069, -0.791,  0.117,  1.274]))\n",
            "0\n",
            "Experience(state=array([-0.069, -0.791,  0.117,  1.274]), action=0, reward=1.0, done=False, new_state=array([-0.085, -0.987,  0.142,  1.601]))\n",
            "1\n",
            "Experience(state=array([-0.085, -0.987,  0.142,  1.601]), action=1, reward=1.0, done=False, new_state=array([-0.105, -0.794,  0.174,  1.356]))\n",
            "0\n",
            "Experience(state=array([-0.105, -0.794,  0.174,  1.356]), action=1, reward=1.0, done=False, new_state=array([-0.121, -0.601,  0.201,  1.123]))\n",
            "1\n",
            "Experience(state=array([-0.121, -0.601,  0.201,  1.123]), action=0, reward=1.0, done=True, new_state=array([-0.133, -0.798,  0.224,  1.471]))\n",
            "0\n",
            "Experience(state=array([-0.033, -0.015, -0.03 ,  0.027]), action=0, reward=1.0, done=False, new_state=array([-0.033, -0.21 , -0.03 ,  0.31 ]))\n",
            "1\n",
            "Experience(state=array([-0.033, -0.21 , -0.03 ,  0.31 ]), action=1, reward=1.0, done=False, new_state=array([-0.037, -0.014, -0.023,  0.008]))\n",
            "0\n",
            "Experience(state=array([-0.037, -0.014, -0.023,  0.008]), action=0, reward=1.0, done=False, new_state=array([-0.038, -0.209, -0.023,  0.293]))\n",
            "1\n",
            "Experience(state=array([-0.038, -0.209, -0.023,  0.293]), action=0, reward=1.0, done=False, new_state=array([-0.042, -0.404, -0.017,  0.578]))\n",
            "0\n",
            "Experience(state=array([-0.042, -0.404, -0.017,  0.578]), action=0, reward=1.0, done=False, new_state=array([-0.05 , -0.599, -0.006,  0.865]))\n",
            "0\n",
            "Experience(state=array([-0.05 , -0.599, -0.006,  0.865]), action=1, reward=1.0, done=False, new_state=array([-0.062, -0.403,  0.011,  0.571]))\n",
            "0\n",
            "Experience(state=array([-0.062, -0.403,  0.011,  0.571]), action=0, reward=1.0, done=False, new_state=array([-0.07 , -0.599,  0.023,  0.867]))\n",
            "0\n",
            "Experience(state=array([-0.07 , -0.599,  0.023,  0.867]), action=1, reward=1.0, done=False, new_state=array([-0.082, -0.404,  0.04 ,  0.582]))\n",
            "0\n",
            "Experience(state=array([-0.082, -0.404,  0.04 ,  0.582]), action=0, reward=1.0, done=False, new_state=array([-0.09 , -0.6  ,  0.052,  0.887]))\n",
            "0\n",
            "Experience(state=array([-0.09 , -0.6  ,  0.052,  0.887]), action=0, reward=1.0, done=False, new_state=array([-0.102, -0.795,  0.07 ,  1.195]))\n",
            "1\n",
            "Experience(state=array([-0.102, -0.795,  0.07 ,  1.195]), action=0, reward=1.0, done=False, new_state=array([-0.118, -0.991,  0.093,  1.509]))\n",
            "1\n",
            "Experience(state=array([-0.118, -0.991,  0.093,  1.509]), action=1, reward=1.0, done=False, new_state=array([-0.138, -0.797,  0.124,  1.247]))\n",
            "1\n",
            "Experience(state=array([-0.138, -0.797,  0.124,  1.247]), action=0, reward=1.0, done=False, new_state=array([-0.154, -0.994,  0.149,  1.575]))\n",
            "0\n",
            "Experience(state=array([-0.154, -0.994,  0.149,  1.575]), action=1, reward=1.0, done=False, new_state=array([-0.174, -0.801,  0.18 ,  1.333]))\n",
            "0\n",
            "Experience(state=array([-0.174, -0.801,  0.18 ,  1.333]), action=1, reward=1.0, done=False, new_state=array([-0.19 , -0.608,  0.207,  1.101]))\n",
            "0\n",
            "Experience(state=array([-0.19 , -0.608,  0.207,  1.101]), action=1, reward=1.0, done=True, new_state=array([-0.202, -0.417,  0.229,  0.88 ]))\n",
            "Your warm-up method seems to function properly, good job!\n"
          ]
        }
      ],
      "source": [
        "# Do not edit - Assertion cell #\n",
        "if should_assert:\n",
        "    parameters = {\"buffer_size\": 1000,\n",
        "                  \"batch_size\": 1}\n",
        "    mock_env1 = gym.make(\"CartPole-v1\")\n",
        "    mock_experience_replay = ExperienceReplay(mock_env1, parameters)\n",
        "    mock_env1.reset()\n",
        "    mock_env2 = deepcopy(mock_env1)\n",
        "\n",
        "    mock_experience_replay.warm_up()\n",
        "    mock_state = mock_env2.reset()\n",
        "\n",
        "    while 0 < len(mock_experience_replay.experience_buffer):\n",
        "        mock_action = mock_env2.action_space.sample()\n",
        "        mock_new_state, mock_reward, mock_done, _ = mock_env2.step(mock_action)\n",
        "\n",
        "        \n",
        "        if mock_done:\n",
        "            mock_state = mock_env2.reset()\n",
        "        else:\n",
        "            mock_state = mock_new_state\n",
        "            \n",
        "    print(\"Your warm-up method seems to function properly, good job!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADuPR6AzcO7"
      },
      "source": [
        "## Create Agent\n",
        "\n",
        "### Target network\n",
        "\n",
        "In the supervised learning example, where we predict cat or dog, the labels are stationary, e.g. an image of a cat will always have the label cat. This is not the case in Reinforcement learning.\n",
        "\n",
        "As we experienced in the previous notebook, the Q-value of the next states is calculated based on the assumption that the Q-table is correct. However, since we begin each training session with a blank table and update the values as we go along, the Q-values of the next state is just an estimate. As a result, we are chasing a moving target, which degrades the training of the neural network and may lead to divergence.\n",
        "\n",
        "Mnih et. al. solved this by using a target network to choose action and to predict the Q-values of the next state $S_{t+1}$, and used a local network to predict the Q-values of state $S_{t}$. The two networks have an identical structure. To update the target network, the equation below is used, where $\\theta$ represents the weights of each network.\n",
        "\n",
        "$\n",
        "\\theta_{target} = \\tau * \\theta_{local} + (1-\\tau) * \\theta_{target}\n",
        "$\n",
        "\n",
        "### Task 1 - Target network\n",
        "Implement the target network algorithm in the \"update_target_network\" method below. We have already created the shell, where we loop through each layer of the network. Your task\n",
        "- Update the weights of each layer by using the algorithm above. Remember to use the self.tau parameter.\n",
        "- Remember to run the assertion cell\n",
        "\n",
        "\n",
        "### Task 2 - Q-learning algorithm\n",
        "Go through the Q-learning algorithm, and compare it to the Q-learning algorithm we used in the previous notebook. Make sure you understand the implementation before moving on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfKeXDtgzcO7"
      },
      "outputs": [],
      "source": [
        "class Agent:    \n",
        "\n",
        "    def __init__(self, env, parameters):\n",
        "        self.env = env\n",
        "        self.q_network = QNetwork(env, parameters)\n",
        "        self.local_network = self.q_network.build_q_network()\n",
        "        self.target_network = self.q_network.build_q_network()\n",
        "        self.experience_replay = ExperienceReplay(env, parameters)\n",
        "        \n",
        "        self.epsilon = parameters[\"epsilon_init\"]\n",
        "        self.epsilon_decay = parameters[\"epsilon_decay\"]\n",
        "        self.epsilon_minimum = parameters[\"epsilon_minimum\"]\n",
        "        self.tau = parameters[\"tau\"]\n",
        "        self.gamma = parameters[\"gamma\"]\n",
        "        self.epochs = parameters[\"epochs\"]\n",
        "\n",
        "    def update_local_network(self):\n",
        "        states, actions, rewards, dones, next_states = self.experience_replay.get_batch()\n",
        "        \n",
        "        # Get Q-values for the next state, Q(next_state), using the target network\n",
        "        Q_target = self.target_network.predict(next_states)\n",
        "\n",
        "        # Apply Q-learning algorithm and Q-value for next state to calculate the actual Q-value the Q(state)\n",
        "        Q_calc = rewards + (self.gamma * np.amax(Q_target, axis=1).reshape(-1, 1) * (1 - dones))\n",
        "        \n",
        "        # Calculate Q-value Q(state) we predicted earlier using the local network\n",
        "        Q_local = self.local_network.predict(states)\n",
        "        \n",
        "        # Update Q_values with \"correct\" Q-values calculated using the Q-learning algorithm      \n",
        "        for row, col_id in enumerate(actions):\n",
        "            Q_local[row, np.asscalar(col_id)] = Q_calc[row]\n",
        "        \n",
        "        # Train network by minimizing the difference between Q_local and modified Q_local\n",
        "        self.local_network.fit(states, Q_local, epochs=self.epochs, verbose=0)\n",
        "    \n",
        "    def update_target_network(self):\n",
        "        local_weights = self.local_network.get_weights()\n",
        "        target_weights = self.target_network.get_weights()\n",
        "\n",
        "        for layer in range(len(local_weights)):\n",
        "            \n",
        "            \"Input code below\"\n",
        "        \n",
        "            \"Input code above\"\n",
        "        \n",
        "        self.target_network.set_weights(target_weights)\n",
        "    \n",
        "    def update_epsilon(self):\n",
        "        if self.epsilon >= self.epsilon_minimum:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        \n",
        "    def select_action(self, state):\n",
        "    \n",
        "        if self.epsilon > np.random.uniform():\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(self.local_network.predict(np.array([state])))\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def step(self, state):\n",
        "        \n",
        "        action  = self.select_action(state)\n",
        "        new_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        return action, reward, done, new_state\n",
        "    \n",
        "    def save(self):\n",
        "        save_dir = os.path.join(os.getcwd(), \n",
        "                                self.env.spec.id +\"_\"+ datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "        os.makedirs(save_dir)\n",
        "        self.target_network.save(\"target_network.h5\")\n",
        "        self.local_network.save(\"local_network.h5\")\n",
        "        print(\"Weights saved successfully\")\n",
        "        \n",
        "    def load(self):\n",
        "        uri = \"freeze_weights/\" + self.env.spec.id + \"/\"\n",
        "        self.target_network.load_weights(uri + \"target_network.h5\")\n",
        "        self.local_network.load_weights(uri + \"local_network.h5\")\n",
        "        print(\"Weights loaded successfully for environment: {}\".format(self.env.spec.id))\n",
        "        \n",
        "    def freeze_network(self, freeze_layers):\n",
        "        # Freezes first freeze_layers of the network and resets the other if freeze_layers is not zero.\n",
        "        network_size = len(self.local_network.get_config().get(\"layers\"))\n",
        "        assert(freeze_layers <= network_size),\\\n",
        "        \"Tried to freeze more layers than there are layers in local network!\"\n",
        "        \n",
        "        self.load()\n",
        "        \n",
        "        if freeze_layers == 0:\n",
        "            print(\"No layers frozen, using loaded weights\")\n",
        "        elif freeze_layers == network_size:\n",
        "            for layer in range(network_size):\n",
        "                self.local_network.layers[layer].trainable = False \n",
        "            print(\"Frozen all layers, and using loaded weights\")\n",
        "        else:             \n",
        "            for layer in range(network_size):\n",
        "                if layer < freeze_layers:\n",
        "                    self.local_network.layers[layer].trainable = False \n",
        "                else:\n",
        "                    new_weights = generate_blank_weights(self.local_network.layers[layer].get_weights()[0].shape)\n",
        "                    self.local_network.layers[layer].set_weights(new_weights)\n",
        "                    self.target_network.layers[layer].set_weights(new_weights)    \n",
        "            \n",
        "            self.local_network.compile(loss=self.q_network.loss_metric, \n",
        "                          optimizer=Adam(lr=self.q_network.learning_rate, \n",
        "                                         decay=self.q_network.learning_rate_decay))\n",
        "            print(\"Networks first {} layers are succesfully frozen\".format(freeze_layers))\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "113216v1zcO8",
        "outputId": "f5ee48f0-7537-4f45-e24b-18fecc6e5630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great job, you implemented the update_target_network-method correctly!\n"
          ]
        }
      ],
      "source": [
        "# Do not edit - Assertion cell #\n",
        "if should_assert:\n",
        "    dummy_parameters, dummy_env = get_dummy_parameters_and_env()\n",
        "    dummy_dqn = Agent(dummy_env, dummy_parameters)\n",
        "\n",
        "    dummy_local_weights = deepcopy(dummy_dqn.local_network.get_weights())\n",
        "    dummy_target_weights = deepcopy(dummy_dqn.target_network.get_weights())\n",
        "\n",
        "    dummy_dqn.update_target_network()\n",
        "    for i in range(len(dummy_target_weights)):\n",
        "        np.testing.assert_array_equal(\n",
        "            dummy_dqn.target_network.get_weights()[i], \n",
        "            dummy_parameters.get(\"tau\") * dummy_local_weights[i] + (1 - dummy_parameters.get(\"tau\")) * dummy_target_weights[i],\n",
        "            err_msg=\"\\nThe target network was not implemented correctly. Layer {} was incorrect\\n\".format(i))\n",
        "\n",
        "    print(\"Great job, you implemented the update_target_network-method correctly!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPqk6IbMzcO8"
      },
      "source": [
        "## Train\n",
        "\n",
        "The training method is more or less identical to the method we used in the previous notebook, with some modifications due to the introduction of a neural network to represent the Q-table.\n",
        "\n",
        "\n",
        "### Task\n",
        "Make sure you understand the method before moving on!\n",
        "- Do you understand why the episodic reward is a better metric of the neural network's performance than the loss-metric when doing Reinforcement learning?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR3X9LxIzcO8"
      },
      "outputs": [],
      "source": [
        "def train(agent, iterations, episodes):\n",
        "    \n",
        "    total_reward = 0\n",
        "    total_reward_list, iterations_list = [], []\n",
        "    agent.experience_replay.warm_up()\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        \n",
        "        state = env.reset()\n",
        "        total_reward=0\n",
        "        \n",
        "        if (episode != 0): \n",
        "            agent.update_epsilon()\n",
        "    \n",
        "        for iteration in range(iterations):\n",
        "            \n",
        "            action, reward, done, new_state = agent.step(state)\n",
        "            agent.experience_replay.add_experience(state, action, reward, done, new_state)\n",
        "            \n",
        "            state = new_state\n",
        "            \n",
        "            agent.update_local_network()\n",
        "            agent.update_target_network()\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done: \n",
        "                break\n",
        "        \n",
        "        total_reward_list.append(total_reward)\n",
        "        iterations_list.append(iteration+1)\n",
        "        \n",
        "        if episode % 5 == 0 and episode != 0:\n",
        "            print(\"Episode: {0:d}-{1:d} | Avg. iterations: {2:0.2f}  | Max total reward: {3:0.2f} | Avg. total reward: {4:0.2f} | Epsilon: {5:0.4f}\" \\\n",
        "                  .format(episode-10, episode, mean(iterations_list), max(total_reward_list), mean(total_reward_list), agent.epsilon))\n",
        "            total_reward_list.clear()\n",
        "            iterations_list.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve6MzLE6zcO8"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "The hyperparameters are the customizable parameters which are not optimized by the agent, but are set by us.\n",
        "\n",
        "The number of parameters passed to the agent significantly increases when we begin using neural networks. To simplify the process, we define them in a dictionary and simply pass the dictionary to the agent. We have chosen the hyperparameters for some environments, and they should not be changed the first time you go through the notebook.\n",
        "\n",
        "PS. You may change them as you please when you have passed through the notebook once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzLaG-fazcO9"
      },
      "outputs": [],
      "source": [
        "def get_hyperparameters(env):\n",
        "    \n",
        "    env_id = env.spec.id\n",
        "        \n",
        "    if env_id == \"CartPole-v1\":\n",
        "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v1\"))\n",
        "        parameters = {\n",
        "            \"tau\" : 0.05,\n",
        "            \"gamma\" : 0.99,\n",
        "            \"epsilon_init\" : 1,\n",
        "            \"epsilon_decay\" : 0.95,\n",
        "            \"epsilon_minimum\": 0.01,\n",
        "            \"buffer_size\" : 2000,\n",
        "            \"batch_size\" : 64,\n",
        "            \"epochs\": 1,\n",
        "            \"loss_metric\" : \"mse\",\n",
        "            \"learning_rate\" : 0.01,\n",
        "            \"learning_rate_decay\": 0.01,\n",
        "            \"hidden_layer_1\": 24,\n",
        "            \"hidden_layer_2\": 24}\n",
        "        \n",
        "    else:\n",
        "        print(\"Standard hyperparameters chosen!\")\n",
        "        parameters = {\n",
        "            \"tau\" : 0.05,\n",
        "            \"gamma\" : 0.95,\n",
        "            \"epsilon_init\" : 1,\n",
        "            \"epsilon_decay\" : 0.97,\n",
        "            \"epsilon_minimum\": 0.01,\n",
        "            \"buffer_size\" : 10000,\n",
        "            \"batch_size\" : 32,\n",
        "            \"epochs\": 1,\n",
        "            \"loss_metric\" : \"mse\",\n",
        "            \"learning_rate\" : 0.01,\n",
        "            \"learning_rate_decay\": 0.01,\n",
        "            \"hidden_layer_1\": 24,\n",
        "            \"hidden_layer_2\": 24}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf24FGxJzcO9"
      },
      "source": [
        "# Environment\n",
        "For this walkthorugh we are going to use an environment called CartPole-v1.\n",
        "\n",
        "<img src=\"https://github.com/acntech/reinforcement-learning-workshop/blob/master/Workshop/Images/CartPole-v1.gif?raw=1\" alt=\"drawing\" width=\"400\" height=\"200\"/>\n",
        "\n",
        "The pendulum starts upright, and the goal is to prevent it from falling over by applying a horizontal force to the cart, of either +1 or -1. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, the cart moves more than 2.4 units from the center, or the agent manages to stay \"alive\" for 500 iterations.\n",
        "\n",
        "### Task\n",
        "- Create an agent by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzNOYRxpzcO9",
        "outputId": "9eaed995-d1a3-451a-f5bb-47a43eeab1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters for CartPole-v1 chosen!\n"
          ]
        }
      ],
      "source": [
        "environment = \"CartPole-v1\"\n",
        "env = gym.make(environment)\n",
        "parameters = get_hyperparameters(env)\n",
        "\n",
        "dqn_agent = Agent(env, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR45aa0fzcO9"
      },
      "source": [
        "## Play\n",
        "To run a single episode using our agent we use a method we created for you called play(), which takes an Agent-object as input. The logic in the play method is identical to the method we used in the previous notebook.\n",
        "\n",
        "### Task\n",
        "- Test the performance of our agent before any training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpddlvDyzcO9",
        "outputId": "cbe7f629-268d-4ddb-daa7-94aa29cc6365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: 9.0\n"
          ]
        }
      ],
      "source": [
        "play(dqn_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNUUI5XzcO-"
      },
      "source": [
        "## Visualize performance\n",
        "We can also visualize the performance of the agent by using another method we created for you, called generate_video(), which takes an Agent-object as input. \n",
        "\n",
        "Since the workshop is hosted on a headless servers, i.e. an EC2 instance on AWS, we need to make some hacks to visualize the performance. The generate_video() method will therefore first generate an .mp4-file, which we will play using the HTML snippet below.\n",
        "\n",
        "### Task\n",
        "- Visualize the performance of the agent by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "zb-9jG7mzcO-",
        "outputId": "c982d255-0da5-46c2-8ca2-717c6511fd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: 8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_video(dqn_agent)\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqzGZipNzcO-"
      },
      "source": [
        "## Train Agent\n",
        "\n",
        "We will begin training our agent for 300 episodes, with a maximum iteration of 500. This will take some time, so be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ip1t_IazcO-",
        "outputId": "c0364e50-d3c3-4126-b916-d78c490633dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: -5-5 | Avg. iterations: 64.33  | Max total reward: 80.00 | Avg. total reward: 64.33 | Epsilon: 0.1748\n",
            "Episode: 0-10 | Avg. iterations: 65.20  | Max total reward: 97.00 | Avg. total reward: 65.20 | Epsilon: 0.1353\n",
            "Episode: 5-15 | Avg. iterations: 67.40  | Max total reward: 75.00 | Avg. total reward: 67.40 | Epsilon: 0.1047\n",
            "Episode: 10-20 | Avg. iterations: 90.20  | Max total reward: 172.00 | Avg. total reward: 90.20 | Epsilon: 0.0810\n",
            "Episode: 15-25 | Avg. iterations: 81.80  | Max total reward: 100.00 | Avg. total reward: 81.80 | Epsilon: 0.0627\n",
            "Episode: 20-30 | Avg. iterations: 96.40  | Max total reward: 125.00 | Avg. total reward: 96.40 | Epsilon: 0.0485\n",
            "Episode: 25-35 | Avg. iterations: 86.20  | Max total reward: 110.00 | Avg. total reward: 86.20 | Epsilon: 0.0375\n",
            "Episode: 30-40 | Avg. iterations: 104.00  | Max total reward: 127.00 | Avg. total reward: 104.00 | Epsilon: 0.0290\n",
            "Episode: 35-45 | Avg. iterations: 109.80  | Max total reward: 180.00 | Avg. total reward: 109.80 | Epsilon: 0.0225\n",
            "Episode: 40-50 | Avg. iterations: 149.20  | Max total reward: 260.00 | Avg. total reward: 149.20 | Epsilon: 0.0174\n",
            "Episode: 45-55 | Avg. iterations: 152.40  | Max total reward: 179.00 | Avg. total reward: 152.40 | Epsilon: 0.0135\n",
            "Episode: 50-60 | Avg. iterations: 100.00  | Max total reward: 108.00 | Avg. total reward: 100.00 | Epsilon: 0.0104\n",
            "Episode: 55-65 | Avg. iterations: 95.60  | Max total reward: 106.00 | Avg. total reward: 95.60 | Epsilon: 0.0099\n",
            "Episode: 60-70 | Avg. iterations: 107.40  | Max total reward: 129.00 | Avg. total reward: 107.40 | Epsilon: 0.0099\n",
            "Episode: 65-75 | Avg. iterations: 122.20  | Max total reward: 149.00 | Avg. total reward: 122.20 | Epsilon: 0.0099\n",
            "Episode: 70-80 | Avg. iterations: 201.20  | Max total reward: 373.00 | Avg. total reward: 201.20 | Epsilon: 0.0099\n",
            "Episode: 75-85 | Avg. iterations: 170.80  | Max total reward: 210.00 | Avg. total reward: 170.80 | Epsilon: 0.0099\n",
            "Episode: 80-90 | Avg. iterations: 181.40  | Max total reward: 259.00 | Avg. total reward: 181.40 | Epsilon: 0.0099\n",
            "Episode: 85-95 | Avg. iterations: 159.40  | Max total reward: 193.00 | Avg. total reward: 159.40 | Epsilon: 0.0099\n"
          ]
        }
      ],
      "source": [
        "episodes = 100\n",
        "iterations = 500\n",
        "\n",
        "train(dqn_agent, iterations, episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUyq0gDQzcO-"
      },
      "source": [
        "## Visualize performance\n",
        "Now lets visualize the performance of our trained agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "ni--Lbb6zcO-",
        "outputId": "793b7569-9f51-4998-e1ec-bc5973286d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_video(dqn_agent)\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7WUHSHEzcO_"
      },
      "source": [
        "## And we are through!\n",
        "\n",
        "You have implemented the DQN-algorithm, congratulations!\n",
        "\n",
        "Try optimizing the hyperparameters to improve the training results. Suggestions:\n",
        "- Number of layers\n",
        "- Number of neurons in each layer\n",
        "- Learning rate\n",
        "- Epsilon strategy\n",
        "\n",
        "If you like to try a different environment, try the \"Acrobat-v1\" environment.\n",
        "\n",
        "Remember to turn of the assertion by running the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNiWE_RvzcO_"
      },
      "outputs": [],
      "source": [
        "should_assert = False"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of Workshop_DQN_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}